{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from skfeature.function.similarity_based import fisher_score\n",
    "\n",
    "# pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13300000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12250000</td>\n",
       "      <td>8960</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12250000</td>\n",
       "      <td>9960</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>semi-furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12215000</td>\n",
       "      <td>7500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11410000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  area  bedrooms  bathrooms  stories mainroad guestroom basement  \\\n",
       "0  13300000  7420         4          2        3      yes        no       no   \n",
       "1  12250000  8960         4          4        4      yes        no       no   \n",
       "2  12250000  9960         3          2        2      yes        no      yes   \n",
       "3  12215000  7500         4          2        2      yes        no      yes   \n",
       "4  11410000  7420         4          1        2      yes       yes      yes   \n",
       "\n",
       "  hotwaterheating airconditioning  parking prefarea furnishingstatus  \n",
       "0              no             yes        2      yes        furnished  \n",
       "1              no             yes        3       no        furnished  \n",
       "2              no              no        2      yes   semi-furnished  \n",
       "3              no             yes        3      yes        furnished  \n",
       "4              no             yes        2       no        furnished  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"Housing.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We have missing values and non numbers</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price               0\n",
       "area                0\n",
       "bedrooms            0\n",
       "bathrooms           0\n",
       "stories             0\n",
       "mainroad            0\n",
       "guestroom           0\n",
       "basement            0\n",
       "hotwaterheating     0\n",
       "airconditioning     0\n",
       "parking             0\n",
       "prefarea            0\n",
       "furnishingstatus    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this just converts the value of column to 0 or 1\n",
    "# factorize in pandas works too, but only one column at a time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "variables = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
    "encoder = LabelEncoder()\n",
    "df[variables] = df[variables].apply(encoder.fit_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13300000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12250000</td>\n",
       "      <td>8960</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12250000</td>\n",
       "      <td>9960</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>semi-furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12215000</td>\n",
       "      <td>7500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11410000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  area  bedrooms  bathrooms  stories  mainroad  guestroom  \\\n",
       "0  13300000  7420         4          2        3         1          0   \n",
       "1  12250000  8960         4          4        4         1          0   \n",
       "2  12250000  9960         3          2        2         1          0   \n",
       "3  12215000  7500         4          2        2         1          0   \n",
       "4  11410000  7420         4          1        2         1          1   \n",
       "\n",
       "   basement  hotwaterheating  airconditioning  parking  prefarea  \\\n",
       "0         0                0                1        2         1   \n",
       "1         0                0                1        3         0   \n",
       "2         1                0                0        2         1   \n",
       "3         1                0                1        3         1   \n",
       "4         1                0                1        2         0   \n",
       "\n",
       "  furnishingstatus  \n",
       "0        furnished  \n",
       "1        furnished  \n",
       "2   semi-furnished  \n",
       "3        furnished  \n",
       "4        furnished  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes multiple columns with the variable (Separate for yes/no)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "variables = ['furnishingstatus']\n",
    "\n",
    "# use encoder\n",
    "encoder = OneHotEncoder(sparse_output=False).set_output(transform=\"pandas\")\n",
    "one_hot_encoded = encoder.fit_transform(df[variables]).astype(int)\n",
    "df = pd.concat([df,one_hot_encoded],axis=1).drop(columns=variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus_furnished</th>\n",
       "      <th>furnishingstatus_semi-furnished</th>\n",
       "      <th>furnishingstatus_unfurnished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13300000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12250000</td>\n",
       "      <td>8960</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12250000</td>\n",
       "      <td>9960</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12215000</td>\n",
       "      <td>7500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11410000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  area  bedrooms  bathrooms  stories  mainroad  guestroom  \\\n",
       "0  13300000  7420         4          2        3         1          0   \n",
       "1  12250000  8960         4          4        4         1          0   \n",
       "2  12250000  9960         3          2        2         1          0   \n",
       "3  12215000  7500         4          2        2         1          0   \n",
       "4  11410000  7420         4          1        2         1          1   \n",
       "\n",
       "   basement  hotwaterheating  airconditioning  parking  prefarea  \\\n",
       "0         0                0                1        2         1   \n",
       "1         0                0                1        3         0   \n",
       "2         1                0                0        2         1   \n",
       "3         1                0                1        3         1   \n",
       "4         1                0                1        2         0   \n",
       "\n",
       "   furnishingstatus_furnished  furnishingstatus_semi-furnished  \\\n",
       "0                           1                                0   \n",
       "1                           1                                0   \n",
       "2                           0                                1   \n",
       "3                           1                                0   \n",
       "4                           1                                0   \n",
       "\n",
       "   furnishingstatus_unfurnished  \n",
       "0                             0  \n",
       "1                             0  \n",
       "2                             0  \n",
       "3                             0  \n",
       "4                             0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('furnishingstatus_unfurnished', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus_furnished</th>\n",
       "      <th>furnishingstatus_semi-furnished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13300000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12250000</td>\n",
       "      <td>8960</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12250000</td>\n",
       "      <td>9960</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12215000</td>\n",
       "      <td>7500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11410000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  area  bedrooms  bathrooms  stories  mainroad  guestroom  \\\n",
       "0  13300000  7420         4          2        3         1          0   \n",
       "1  12250000  8960         4          4        4         1          0   \n",
       "2  12250000  9960         3          2        2         1          0   \n",
       "3  12215000  7500         4          2        2         1          0   \n",
       "4  11410000  7420         4          1        2         1          1   \n",
       "\n",
       "   basement  hotwaterheating  airconditioning  parking  prefarea  \\\n",
       "0         0                0                1        2         1   \n",
       "1         0                0                1        3         0   \n",
       "2         1                0                0        2         1   \n",
       "3         1                0                1        3         1   \n",
       "4         1                0                1        2         0   \n",
       "\n",
       "   furnishingstatus_furnished  furnishingstatus_semi-furnished  \n",
       "0                           1                                0  \n",
       "1                           1                                0  \n",
       "2                           0                                1  \n",
       "3                           1                                0  \n",
       "4                           1                                0  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "    \n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'mainroad',\n",
       "       'guestroom', 'basement', 'hotwaterheating', 'airconditioning',\n",
       "       'parking', 'prefarea', 'furnishingstatus_furnished',\n",
       "       'furnishingstatus_semi-furnished'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus_furnished</th>\n",
       "      <th>furnishingstatus_semi-furnished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.450000e+02</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>545.000000</td>\n",
       "      <td>545.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.766729e+06</td>\n",
       "      <td>5150.541284</td>\n",
       "      <td>2.965138</td>\n",
       "      <td>1.286239</td>\n",
       "      <td>1.805505</td>\n",
       "      <td>0.858716</td>\n",
       "      <td>0.177982</td>\n",
       "      <td>0.350459</td>\n",
       "      <td>0.045872</td>\n",
       "      <td>0.315596</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>0.234862</td>\n",
       "      <td>0.256881</td>\n",
       "      <td>0.416514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.870440e+06</td>\n",
       "      <td>2170.141023</td>\n",
       "      <td>0.738064</td>\n",
       "      <td>0.502470</td>\n",
       "      <td>0.867492</td>\n",
       "      <td>0.348635</td>\n",
       "      <td>0.382849</td>\n",
       "      <td>0.477552</td>\n",
       "      <td>0.209399</td>\n",
       "      <td>0.465180</td>\n",
       "      <td>0.861586</td>\n",
       "      <td>0.424302</td>\n",
       "      <td>0.437314</td>\n",
       "      <td>0.493434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.750000e+06</td>\n",
       "      <td>1650.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.430000e+06</td>\n",
       "      <td>3600.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.340000e+06</td>\n",
       "      <td>4600.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.740000e+06</td>\n",
       "      <td>6360.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.330000e+07</td>\n",
       "      <td>16200.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price          area    bedrooms   bathrooms     stories  \\\n",
       "count  5.450000e+02    545.000000  545.000000  545.000000  545.000000   \n",
       "mean   4.766729e+06   5150.541284    2.965138    1.286239    1.805505   \n",
       "std    1.870440e+06   2170.141023    0.738064    0.502470    0.867492   \n",
       "min    1.750000e+06   1650.000000    1.000000    1.000000    1.000000   \n",
       "25%    3.430000e+06   3600.000000    2.000000    1.000000    1.000000   \n",
       "50%    4.340000e+06   4600.000000    3.000000    1.000000    2.000000   \n",
       "75%    5.740000e+06   6360.000000    3.000000    2.000000    2.000000   \n",
       "max    1.330000e+07  16200.000000    6.000000    4.000000    4.000000   \n",
       "\n",
       "         mainroad   guestroom    basement  hotwaterheating  airconditioning  \\\n",
       "count  545.000000  545.000000  545.000000       545.000000       545.000000   \n",
       "mean     0.858716    0.177982    0.350459         0.045872         0.315596   \n",
       "std      0.348635    0.382849    0.477552         0.209399         0.465180   \n",
       "min      0.000000    0.000000    0.000000         0.000000         0.000000   \n",
       "25%      1.000000    0.000000    0.000000         0.000000         0.000000   \n",
       "50%      1.000000    0.000000    0.000000         0.000000         0.000000   \n",
       "75%      1.000000    0.000000    1.000000         0.000000         1.000000   \n",
       "max      1.000000    1.000000    1.000000         1.000000         1.000000   \n",
       "\n",
       "          parking    prefarea  furnishingstatus_furnished  \\\n",
       "count  545.000000  545.000000                  545.000000   \n",
       "mean     0.693578    0.234862                    0.256881   \n",
       "std      0.861586    0.424302                    0.437314   \n",
       "min      0.000000    0.000000                    0.000000   \n",
       "25%      0.000000    0.000000                    0.000000   \n",
       "50%      0.000000    0.000000                    0.000000   \n",
       "75%      1.000000    0.000000                    1.000000   \n",
       "max      3.000000    1.000000                    1.000000   \n",
       "\n",
       "       furnishingstatus_semi-furnished  \n",
       "count                       545.000000  \n",
       "mean                          0.416514  \n",
       "std                           0.493434  \n",
       "min                           0.000000  \n",
       "25%                           0.000000  \n",
       "50%                           0.000000  \n",
       "75%                           1.000000  \n",
       "max                           1.000000  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['area', 'bedrooms', 'bathrooms', 'stories', \n",
    "       'guestroom', 'basement', 'hotwaterheating', 'airconditioning',\n",
    "       'parking', 'prefarea', 'furnishingstatus_furnished',\n",
    "       'furnishingstatus_semi-furnished']]\n",
    "#out of the picture\n",
    "#'mainroad',\n",
    "\n",
    "y=df['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus_furnished</th>\n",
       "      <th>furnishingstatus_semi-furnished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.535997</td>\n",
       "      <td>0.366494</td>\n",
       "      <td>0.517545</td>\n",
       "      <td>0.420712</td>\n",
       "      <td>0.296898</td>\n",
       "      <td>0.255517</td>\n",
       "      <td>0.187057</td>\n",
       "      <td>0.093073</td>\n",
       "      <td>0.452954</td>\n",
       "      <td>0.384394</td>\n",
       "      <td>0.329777</td>\n",
       "      <td>0.229350</td>\n",
       "      <td>0.063656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area</th>\n",
       "      <td>0.535997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.151858</td>\n",
       "      <td>0.193820</td>\n",
       "      <td>0.083996</td>\n",
       "      <td>0.288874</td>\n",
       "      <td>0.140297</td>\n",
       "      <td>0.047417</td>\n",
       "      <td>-0.009229</td>\n",
       "      <td>0.222393</td>\n",
       "      <td>0.352980</td>\n",
       "      <td>0.234779</td>\n",
       "      <td>0.145772</td>\n",
       "      <td>0.006156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedrooms</th>\n",
       "      <td>0.366494</td>\n",
       "      <td>0.151858</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373930</td>\n",
       "      <td>0.408564</td>\n",
       "      <td>-0.012033</td>\n",
       "      <td>0.080549</td>\n",
       "      <td>0.097312</td>\n",
       "      <td>0.046049</td>\n",
       "      <td>0.160603</td>\n",
       "      <td>0.139270</td>\n",
       "      <td>0.079023</td>\n",
       "      <td>0.079054</td>\n",
       "      <td>0.050040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bathrooms</th>\n",
       "      <td>0.517545</td>\n",
       "      <td>0.193820</td>\n",
       "      <td>0.373930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.326165</td>\n",
       "      <td>0.042398</td>\n",
       "      <td>0.126469</td>\n",
       "      <td>0.102106</td>\n",
       "      <td>0.067159</td>\n",
       "      <td>0.186915</td>\n",
       "      <td>0.177496</td>\n",
       "      <td>0.063472</td>\n",
       "      <td>0.108139</td>\n",
       "      <td>0.029834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stories</th>\n",
       "      <td>0.420712</td>\n",
       "      <td>0.083996</td>\n",
       "      <td>0.408564</td>\n",
       "      <td>0.326165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.121706</td>\n",
       "      <td>0.043538</td>\n",
       "      <td>-0.172394</td>\n",
       "      <td>0.018847</td>\n",
       "      <td>0.293602</td>\n",
       "      <td>0.045547</td>\n",
       "      <td>0.044425</td>\n",
       "      <td>0.093176</td>\n",
       "      <td>-0.003648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mainroad</th>\n",
       "      <td>0.296898</td>\n",
       "      <td>0.288874</td>\n",
       "      <td>-0.012033</td>\n",
       "      <td>0.042398</td>\n",
       "      <td>0.121706</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.092337</td>\n",
       "      <td>0.044002</td>\n",
       "      <td>-0.011781</td>\n",
       "      <td>0.105423</td>\n",
       "      <td>0.204433</td>\n",
       "      <td>0.199876</td>\n",
       "      <td>0.129971</td>\n",
       "      <td>0.011450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guestroom</th>\n",
       "      <td>0.255517</td>\n",
       "      <td>0.140297</td>\n",
       "      <td>0.080549</td>\n",
       "      <td>0.126469</td>\n",
       "      <td>0.043538</td>\n",
       "      <td>0.092337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.372066</td>\n",
       "      <td>-0.010308</td>\n",
       "      <td>0.138179</td>\n",
       "      <td>0.037466</td>\n",
       "      <td>0.160897</td>\n",
       "      <td>0.099721</td>\n",
       "      <td>0.005821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basement</th>\n",
       "      <td>0.187057</td>\n",
       "      <td>0.047417</td>\n",
       "      <td>0.097312</td>\n",
       "      <td>0.102106</td>\n",
       "      <td>-0.172394</td>\n",
       "      <td>0.044002</td>\n",
       "      <td>0.372066</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>0.047341</td>\n",
       "      <td>0.051497</td>\n",
       "      <td>0.228083</td>\n",
       "      <td>0.069852</td>\n",
       "      <td>0.050284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hotwaterheating</th>\n",
       "      <td>0.093073</td>\n",
       "      <td>-0.009229</td>\n",
       "      <td>0.046049</td>\n",
       "      <td>0.067159</td>\n",
       "      <td>0.018847</td>\n",
       "      <td>-0.011781</td>\n",
       "      <td>-0.010308</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.130023</td>\n",
       "      <td>0.067864</td>\n",
       "      <td>-0.059411</td>\n",
       "      <td>-0.008472</td>\n",
       "      <td>0.063819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airconditioning</th>\n",
       "      <td>0.452954</td>\n",
       "      <td>0.222393</td>\n",
       "      <td>0.160603</td>\n",
       "      <td>0.186915</td>\n",
       "      <td>0.293602</td>\n",
       "      <td>0.105423</td>\n",
       "      <td>0.138179</td>\n",
       "      <td>0.047341</td>\n",
       "      <td>-0.130023</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.159173</td>\n",
       "      <td>0.117382</td>\n",
       "      <td>0.160994</td>\n",
       "      <td>-0.053179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parking</th>\n",
       "      <td>0.384394</td>\n",
       "      <td>0.352980</td>\n",
       "      <td>0.139270</td>\n",
       "      <td>0.177496</td>\n",
       "      <td>0.045547</td>\n",
       "      <td>0.204433</td>\n",
       "      <td>0.037466</td>\n",
       "      <td>0.051497</td>\n",
       "      <td>0.067864</td>\n",
       "      <td>0.159173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091627</td>\n",
       "      <td>0.131234</td>\n",
       "      <td>0.041327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prefarea</th>\n",
       "      <td>0.329777</td>\n",
       "      <td>0.234779</td>\n",
       "      <td>0.079023</td>\n",
       "      <td>0.063472</td>\n",
       "      <td>0.044425</td>\n",
       "      <td>0.199876</td>\n",
       "      <td>0.160897</td>\n",
       "      <td>0.228083</td>\n",
       "      <td>-0.059411</td>\n",
       "      <td>0.117382</td>\n",
       "      <td>0.091627</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100249</td>\n",
       "      <td>-0.011535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>furnishingstatus_furnished</th>\n",
       "      <td>0.229350</td>\n",
       "      <td>0.145772</td>\n",
       "      <td>0.079054</td>\n",
       "      <td>0.108139</td>\n",
       "      <td>0.093176</td>\n",
       "      <td>0.129971</td>\n",
       "      <td>0.099721</td>\n",
       "      <td>0.069852</td>\n",
       "      <td>-0.008472</td>\n",
       "      <td>0.160994</td>\n",
       "      <td>0.131234</td>\n",
       "      <td>0.100249</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.496748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>furnishingstatus_semi-furnished</th>\n",
       "      <td>0.063656</td>\n",
       "      <td>0.006156</td>\n",
       "      <td>0.050040</td>\n",
       "      <td>0.029834</td>\n",
       "      <td>-0.003648</td>\n",
       "      <td>0.011450</td>\n",
       "      <td>0.005821</td>\n",
       "      <td>0.050284</td>\n",
       "      <td>0.063819</td>\n",
       "      <td>-0.053179</td>\n",
       "      <td>0.041327</td>\n",
       "      <td>-0.011535</td>\n",
       "      <td>-0.496748</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    price      area  bedrooms  bathrooms  \\\n",
       "price                            1.000000  0.535997  0.366494   0.517545   \n",
       "area                             0.535997  1.000000  0.151858   0.193820   \n",
       "bedrooms                         0.366494  0.151858  1.000000   0.373930   \n",
       "bathrooms                        0.517545  0.193820  0.373930   1.000000   \n",
       "stories                          0.420712  0.083996  0.408564   0.326165   \n",
       "mainroad                         0.296898  0.288874 -0.012033   0.042398   \n",
       "guestroom                        0.255517  0.140297  0.080549   0.126469   \n",
       "basement                         0.187057  0.047417  0.097312   0.102106   \n",
       "hotwaterheating                  0.093073 -0.009229  0.046049   0.067159   \n",
       "airconditioning                  0.452954  0.222393  0.160603   0.186915   \n",
       "parking                          0.384394  0.352980  0.139270   0.177496   \n",
       "prefarea                         0.329777  0.234779  0.079023   0.063472   \n",
       "furnishingstatus_furnished       0.229350  0.145772  0.079054   0.108139   \n",
       "furnishingstatus_semi-furnished  0.063656  0.006156  0.050040   0.029834   \n",
       "\n",
       "                                  stories  mainroad  guestroom  basement  \\\n",
       "price                            0.420712  0.296898   0.255517  0.187057   \n",
       "area                             0.083996  0.288874   0.140297  0.047417   \n",
       "bedrooms                         0.408564 -0.012033   0.080549  0.097312   \n",
       "bathrooms                        0.326165  0.042398   0.126469  0.102106   \n",
       "stories                          1.000000  0.121706   0.043538 -0.172394   \n",
       "mainroad                         0.121706  1.000000   0.092337  0.044002   \n",
       "guestroom                        0.043538  0.092337   1.000000  0.372066   \n",
       "basement                        -0.172394  0.044002   0.372066  1.000000   \n",
       "hotwaterheating                  0.018847 -0.011781  -0.010308  0.004385   \n",
       "airconditioning                  0.293602  0.105423   0.138179  0.047341   \n",
       "parking                          0.045547  0.204433   0.037466  0.051497   \n",
       "prefarea                         0.044425  0.199876   0.160897  0.228083   \n",
       "furnishingstatus_furnished       0.093176  0.129971   0.099721  0.069852   \n",
       "furnishingstatus_semi-furnished -0.003648  0.011450   0.005821  0.050284   \n",
       "\n",
       "                                 hotwaterheating  airconditioning   parking  \\\n",
       "price                                   0.093073         0.452954  0.384394   \n",
       "area                                   -0.009229         0.222393  0.352980   \n",
       "bedrooms                                0.046049         0.160603  0.139270   \n",
       "bathrooms                               0.067159         0.186915  0.177496   \n",
       "stories                                 0.018847         0.293602  0.045547   \n",
       "mainroad                               -0.011781         0.105423  0.204433   \n",
       "guestroom                              -0.010308         0.138179  0.037466   \n",
       "basement                                0.004385         0.047341  0.051497   \n",
       "hotwaterheating                         1.000000        -0.130023  0.067864   \n",
       "airconditioning                        -0.130023         1.000000  0.159173   \n",
       "parking                                 0.067864         0.159173  1.000000   \n",
       "prefarea                               -0.059411         0.117382  0.091627   \n",
       "furnishingstatus_furnished             -0.008472         0.160994  0.131234   \n",
       "furnishingstatus_semi-furnished         0.063819        -0.053179  0.041327   \n",
       "\n",
       "                                 prefarea  furnishingstatus_furnished  \\\n",
       "price                            0.329777                    0.229350   \n",
       "area                             0.234779                    0.145772   \n",
       "bedrooms                         0.079023                    0.079054   \n",
       "bathrooms                        0.063472                    0.108139   \n",
       "stories                          0.044425                    0.093176   \n",
       "mainroad                         0.199876                    0.129971   \n",
       "guestroom                        0.160897                    0.099721   \n",
       "basement                         0.228083                    0.069852   \n",
       "hotwaterheating                 -0.059411                   -0.008472   \n",
       "airconditioning                  0.117382                    0.160994   \n",
       "parking                          0.091627                    0.131234   \n",
       "prefarea                         1.000000                    0.100249   \n",
       "furnishingstatus_furnished       0.100249                    1.000000   \n",
       "furnishingstatus_semi-furnished -0.011535                   -0.496748   \n",
       "\n",
       "                                 furnishingstatus_semi-furnished  \n",
       "price                                                   0.063656  \n",
       "area                                                    0.006156  \n",
       "bedrooms                                                0.050040  \n",
       "bathrooms                                               0.029834  \n",
       "stories                                                -0.003648  \n",
       "mainroad                                                0.011450  \n",
       "guestroom                                               0.005821  \n",
       "basement                                                0.050284  \n",
       "hotwaterheating                                         0.063819  \n",
       "airconditioning                                        -0.053179  \n",
       "parking                                                 0.041327  \n",
       "prefarea                                               -0.011535  \n",
       "furnishingstatus_furnished                             -0.496748  \n",
       "furnishingstatus_semi-furnished                         1.000000  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlations = df.corr()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'row' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ranks \u001b[38;5;241m=\u001b[39m \u001b[43mfisher_score\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfisher_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m feat_importances \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(ranks, X\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m      4\u001b[0m feat_importances\u001b[38;5;241m.\u001b[39mplot(kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbarh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\janni\\OneDrive\\Fächer - Uni\\5.Semester\\DeepL\\Test\\.venv\\lib\\site-packages\\skfeature\\function\\similarity_based\\fisher_score.py:40\u001b[0m, in \u001b[0;36mfisher_score\u001b[1;34m(X, y, mode)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Construct weight matrix W in a fisherScore way\u001b[39;00m\n\u001b[0;32m     39\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneighbor_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupervised\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfisher_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: y}\n\u001b[1;32m---> 40\u001b[0m W \u001b[38;5;241m=\u001b[39m construct_W(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# build the diagonal D matrix from affinity matrix W\u001b[39;00m\n\u001b[0;32m     43\u001b[0m D \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(W\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\janni\\OneDrive\\Fächer - Uni\\5.Semester\\DeepL\\Test\\.venv\\lib\\site-packages\\skfeature\\utility\\construct_W.py:195\u001b[0m, in \u001b[0;36mconstruct_W\u001b[1;34m(X, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m         class_idx \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m==\u001b[39m label[i]\n\u001b[0;32m    194\u001b[0m         class_idx_all \u001b[38;5;241m=\u001b[39m class_idx[:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m&\u001b[39m class_idx[np\u001b[38;5;241m.\u001b[39mnewaxis, :]\n\u001b[1;32m--> 195\u001b[0m         \u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_idx_all\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39msum(class_idx))\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m W\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# construct the weight matrix W in a reliefF way, NH(x) and NM(x,y) denotes a set of k nearest\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# points to x with the same class as x, a different class (the class y), respectively. W_ij = 1 if i = j;\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# W_ij = 1/k if x_j \\in NH(x_i); W_ij = -1/(c-1)k if x_j \\in NM(x_i, y)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\janni\\OneDrive\\Fächer - Uni\\5.Semester\\DeepL\\Test\\.venv\\lib\\site-packages\\scipy\\sparse\\_lil.py:265\u001b[0m, in \u001b[0;36m_lil_base.__setitem__\u001b[1;34m(self, key, x)\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Everything else takes the normal path.\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m \u001b[43mIndexMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\janni\\OneDrive\\Fächer - Uni\\5.Semester\\DeepL\\Test\\.venv\\lib\\site-packages\\scipy\\sparse\\_index.py:102\u001b[0m, in \u001b[0;36mIndexMixin.__setitem__\u001b[1;34m(self, key, x)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, x):\n\u001b[1;32m--> 102\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row, INT_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, INT_TYPES):\n\u001b[0;32m    105\u001b[0m         x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\janni\\OneDrive\\Fächer - Uni\\5.Semester\\DeepL\\Test\\.venv\\lib\\site-packages\\scipy\\sparse\\_index.py:162\u001b[0m, in \u001b[0;36mIndexMixin._validate_indices\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    160\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m \u001b[43m_unpack_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m M, N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_bool_idx\u001b[39m(\n\u001b[0;32m    166\u001b[0m     idx: npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mbool_],\n\u001b[0;32m    167\u001b[0m     axis_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    168\u001b[0m     axis_name: \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m    169\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mint_]:\n",
      "File \u001b[1;32mc:\\Users\\janni\\OneDrive\\Fächer - Uni\\5.Semester\\DeepL\\Test\\.venv\\lib\\site-packages\\scipy\\sparse\\_index.py:324\u001b[0m, in \u001b[0;36m_unpack_index\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# Next, check for validity and transform the index as needed.\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(\u001b[43mrow\u001b[49m) \u001b[38;5;129;01mor\u001b[39;00m issparse(col):\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;66;03m# Supporting sparse boolean indexing with both row and col does\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;66;03m# not work because spmatrix.ndim is always 2.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndexing with sparse matrices is not supported \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexcept boolean indexing where matrix and index \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mare equal shapes.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row, col\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'row' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#Fisher\n",
    "ranks = fisher_score.fisher_score(X.values, y.values)\n",
    "\n",
    "feat_importances = pd.Series(ranks, X.columns)\n",
    "feat_importances.plot(kind='barh')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>area</td>\n",
       "      <td>279446.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>parking</td>\n",
       "      <td>282.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>guestroom</td>\n",
       "      <td>248.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hotwaterheating</td>\n",
       "      <td>224.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>prefarea</td>\n",
       "      <td>223.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>airconditioning</td>\n",
       "      <td>198.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>furnishingstatus_furnished</td>\n",
       "      <td>183.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>basement</td>\n",
       "      <td>171.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>furnishingstatus_semi-furnished</td>\n",
       "      <td>137.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stories</td>\n",
       "      <td>128.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bathrooms</td>\n",
       "      <td>64.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>49.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mainroad</td>\n",
       "      <td>34.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Features     Score\n",
       "0                              area 279446.07\n",
       "9                           parking    282.83\n",
       "5                         guestroom    248.50\n",
       "7                   hotwaterheating    224.40\n",
       "10                         prefarea    223.95\n",
       "8                   airconditioning    198.96\n",
       "11       furnishingstatus_furnished    183.85\n",
       "6                          basement    171.89\n",
       "12  furnishingstatus_semi-furnished    137.84\n",
       "3                           stories    128.58\n",
       "2                         bathrooms     64.33\n",
       "1                          bedrooms     49.67\n",
       "4                          mainroad     34.41"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# convert all continuous variables to integer,\n",
    "# and convert all negative numbers to 0\n",
    "X_cat = X.astype(int)\n",
    "X_cat = X_cat.clip(lower=0)\n",
    "\n",
    "# initialize chi2 and SelectKBest\n",
    "# Note: chi2 -test is a very common test\n",
    "# in statistics and quantitative analysis\n",
    "# basically it studies the data whether variables are related\n",
    "# or independent of each other\n",
    "chi_2_features = SelectKBest(chi2, k=len(X_cat.columns))\n",
    "\n",
    "# fit our data to the SelectKBest\n",
    "best_features = chi_2_features.fit(X_cat,y.astype(int))\n",
    "\n",
    "# use decimal format in table print later\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "# wrap it up, and show the results\n",
    "# the higher the score, the more effect that column has on price\n",
    "df_features = pd.DataFrame(best_features.scores_)\n",
    "df_columns = pd.DataFrame(X_cat.columns)\n",
    "f_scores = pd.concat([df_columns,df_features],axis=1)\n",
    "f_scores.columns = ['Features','Score']\n",
    "f_scores.sort_values(by='Score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X,y, test_size=0.3, random_state=101)\n",
    "\n",
    "X_val, X_test, y_val, y_test=train_test_split(X_temp, y_temp, test_size=0.5, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data amount: 381\n",
      "Test data amount: 82\n",
      "Validation data amount: 82\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train data amount: {len(X_train)}\")\n",
    "print(f\"Test data amount: {len(X_test)}\")\n",
    "print(f\"Validation data amount: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as in Classic ML, it's often a good idea to follow this structure:\n",
    "# Normalization/standardization -> Regularization -> Rest of the model\n",
    "# this example does just that\n",
    "# you can also try LayerNormalization instead of BatchNormalization\n",
    "# and l2-regularizer instead of l1. depends on situation which work better\n",
    "#model = keras.Sequential(\n",
    "#    [\n",
    "#        layers.BatchNormalization(input_shape=(9,)),\n",
    "#        layers.Dense(4, activation=\"relu\", kernel_regularizer=keras.regularizers.l1(l1=0.1)),\n",
    "#        layers.Dense(12, activation=\"relu\"),\n",
    "#        layers.Dropout(0.1),\n",
    "#        layers.Dense(8, activation=\"relu\"),\n",
    "#        layers.Dense(1)\n",
    "#    ]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janni\\OneDrive\\Fächer - Uni\\5.Semester\\DeepL\\Test\\.venv\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:143: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">336</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             │            \u001b[38;5;34m52\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │           \u001b[38;5;34m336\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">933</span> (3.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m933\u001b[0m (3.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">907</span> (3.54 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m907\u001b[0m (3.54 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26</span> (104.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m26\u001b[0m (104.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var_amount=len(X.columns)\n",
    "\n",
    "# needed imports:\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# create a model checkpoint to a file, and only save the best one\n",
    "mc = ModelCheckpoint('best_model.keras', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "# combine all active callbacks into a list\n",
    "# have only those you need, for example only ModelCheckpoint\n",
    "callback_list = [ mc]\n",
    "\n",
    "# later, we need to attach the callbacks right when we start training\n",
    "# model.fit(x=X_train, y=y_train, epochs=3000, validation_data=(X_val, y_val), callbacks=callback_list)\n",
    "\n",
    "# IF USING MODELCHECKPOINT:\n",
    "\n",
    "# we have to load the model after plotting the training history, and right before evaluation metrics:\n",
    "\n",
    "# override the last model with the best model version from the history\n",
    "# from keras.models import load_model\n",
    "# model = load_model('best_model.keras')\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.BatchNormalization(input_shape=(var_amount,)),\n",
    "        layers.Dense(24, activation=\"relu\", kernel_regularizer=keras.regularizers.l1_l2(l1=0.1,l2=0.1)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(16, activation=\"relu\"),\n",
    "        layers.Dense(8, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - loss: 27951003336704.0000 - val_loss: 25236873412608.0000\n",
      "Epoch 2/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23868412526592.0000 - val_loss: 25236867121152.0000\n",
      "Epoch 3/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26253214089216.0000 - val_loss: 25236860829696.0000\n",
      "Epoch 4/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26099268452352.0000 - val_loss: 25236850343936.0000\n",
      "Epoch 5/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 27133212622848.0000 - val_loss: 25236835663872.0000\n",
      "Epoch 6/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 26550164520960.0000 - val_loss: 25236814692352.0000\n",
      "Epoch 7/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24282782498816.0000 - val_loss: 25236779040768.0000\n",
      "Epoch 8/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25348475453440.0000 - val_loss: 25236735000576.0000\n",
      "Epoch 9/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26229147172864.0000 - val_loss: 25236667891712.0000\n",
      "Epoch 10/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25383067975680.0000 - val_loss: 25236575617024.0000\n",
      "Epoch 11/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26605342687232.0000 - val_loss: 25236447690752.0000\n",
      "Epoch 12/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25354079043584.0000 - val_loss: 25236267335680.0000\n",
      "Epoch 13/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 25108248788992.0000 - val_loss: 25236017774592.0000\n",
      "Epoch 14/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 25180080439296.0000 - val_loss: 25235675938816.0000\n",
      "Epoch 15/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24500252966912.0000 - val_loss: 25235229245440.0000\n",
      "Epoch 16/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26983790542848.0000 - val_loss: 25234625265664.0000\n",
      "Epoch 17/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25719222566912.0000 - val_loss: 25233830445056.0000\n",
      "Epoch 18/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 25941212397568.0000 - val_loss: 25232809132032.0000\n",
      "Epoch 19/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 25304458330112.0000 - val_loss: 25231502606336.0000\n",
      "Epoch 20/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24825322012672.0000 - val_loss: 25229839564800.0000\n",
      "Epoch 21/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25960950792192.0000 - val_loss: 25227744509952.0000\n",
      "Epoch 22/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25352241938432.0000 - val_loss: 25225114681344.0000\n",
      "Epoch 23/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 26210065186816.0000 - val_loss: 25221874581504.0000\n",
      "Epoch 24/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 26165444083712.0000 - val_loss: 25217896284160.0000\n",
      "Epoch 25/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 26149461688320.0000 - val_loss: 25213045571584.0000\n",
      "Epoch 26/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 26843610611712.0000 - val_loss: 25207150477312.0000\n",
      "Epoch 27/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25143980064768.0000 - val_loss: 25199940468736.0000\n",
      "Epoch 28/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 26706261835776.0000 - val_loss: 25191168081920.0000\n",
      "Epoch 29/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25126863110144.0000 - val_loss: 25180969631744.0000\n",
      "Epoch 30/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25213012017152.0000 - val_loss: 25168921493504.0000\n",
      "Epoch 31/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25594041466880.0000 - val_loss: 25154715385856.0000\n",
      "Epoch 32/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24933144985600.0000 - val_loss: 25137954947072.0000\n",
      "Epoch 33/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 26153018458112.0000 - val_loss: 25118271078400.0000\n",
      "Epoch 34/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24263794884608.0000 - val_loss: 25095579893760.0000\n",
      "Epoch 35/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26669450526720.0000 - val_loss: 25069092864000.0000\n",
      "Epoch 36/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 25275058356224.0000 - val_loss: 25038965178368.0000\n",
      "Epoch 37/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25810958286848.0000 - val_loss: 25004710297600.0000\n",
      "Epoch 38/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 24756034207744.0000 - val_loss: 24965485166592.0000\n",
      "Epoch 39/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25770384687104.0000 - val_loss: 24920532713472.0000\n",
      "Epoch 40/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 26281682927616.0000 - val_loss: 24869601280000.0000\n",
      "Epoch 41/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26174403117056.0000 - val_loss: 24811759730688.0000\n",
      "Epoch 42/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 26698445750272.0000 - val_loss: 24746680909824.0000\n",
      "Epoch 43/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25004945178624.0000 - val_loss: 24674889105408.0000\n",
      "Epoch 44/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25361305829376.0000 - val_loss: 24594666749952.0000\n",
      "Epoch 45/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25599903006720.0000 - val_loss: 24506066272256.0000\n",
      "Epoch 46/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 24066488532992.0000 - val_loss: 24407674191872.0000\n",
      "Epoch 47/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24319249874944.0000 - val_loss: 24299280793600.0000\n",
      "Epoch 48/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 24848166289408.0000 - val_loss: 24180460355584.0000\n",
      "Epoch 49/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 26893621395456.0000 - val_loss: 24050778767360.0000\n",
      "Epoch 50/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 26066036981760.0000 - val_loss: 23909713838080.0000\n",
      "Epoch 51/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25258583130112.0000 - val_loss: 23756221186048.0000\n",
      "Epoch 52/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23775024250880.0000 - val_loss: 23589367578624.0000\n",
      "Epoch 53/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23831819321344.0000 - val_loss: 23409142530048.0000\n",
      "Epoch 54/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 26474620911616.0000 - val_loss: 23212683427840.0000\n",
      "Epoch 55/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23360421494784.0000 - val_loss: 23004922773504.0000\n",
      "Epoch 56/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25034494050304.0000 - val_loss: 22782140219392.0000\n",
      "Epoch 57/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 23070970478592.0000 - val_loss: 22545055088640.0000\n",
      "Epoch 58/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21545105752064.0000 - val_loss: 22292184694784.0000\n",
      "Epoch 59/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24531609583616.0000 - val_loss: 22018950955008.0000\n",
      "Epoch 60/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23683663921152.0000 - val_loss: 21727557976064.0000\n",
      "Epoch 61/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21569522892800.0000 - val_loss: 21423812771840.0000\n",
      "Epoch 62/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23225270534144.0000 - val_loss: 21100339658752.0000\n",
      "Epoch 63/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 22007527768064.0000 - val_loss: 20758732472320.0000\n",
      "Epoch 64/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 22277540282368.0000 - val_loss: 20401478434816.0000\n",
      "Epoch 65/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18877366927360.0000 - val_loss: 20034493612032.0000\n",
      "Epoch 66/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 21126658916352.0000 - val_loss: 19638809264128.0000\n",
      "Epoch 67/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 20006351929344.0000 - val_loss: 19231089360896.0000\n",
      "Epoch 68/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19259323318272.0000 - val_loss: 18805380087808.0000\n",
      "Epoch 69/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19637127348224.0000 - val_loss: 18356992212992.0000\n",
      "Epoch 70/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19255233871872.0000 - val_loss: 17899135696896.0000\n",
      "Epoch 71/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17771614175232.0000 - val_loss: 17427533398016.0000\n",
      "Epoch 72/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 17072055648256.0000 - val_loss: 16934837944320.0000\n",
      "Epoch 73/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 17024987168768.0000 - val_loss: 16431319089152.0000\n",
      "Epoch 74/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16453864521728.0000 - val_loss: 15913959030784.0000\n",
      "Epoch 75/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 16052975042560.0000 - val_loss: 15388295299072.0000\n",
      "Epoch 76/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 15903434473472.0000 - val_loss: 14848419168256.0000\n",
      "Epoch 77/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 16204897976320.0000 - val_loss: 14295456808960.0000\n",
      "Epoch 78/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 14146296872960.0000 - val_loss: 13743568191488.0000\n",
      "Epoch 79/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12774789873664.0000 - val_loss: 13192864464896.0000\n",
      "Epoch 80/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12243395674112.0000 - val_loss: 12639769985024.0000\n",
      "Epoch 81/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12245760212992.0000 - val_loss: 12070519046144.0000\n",
      "Epoch 82/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 12050933743616.0000 - val_loss: 11505508548608.0000\n",
      "Epoch 83/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10907812888576.0000 - val_loss: 10950807650304.0000\n",
      "Epoch 84/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 10437379751936.0000 - val_loss: 10393408765952.0000\n",
      "Epoch 85/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 9901264863232.0000 - val_loss: 9840655073280.0000\n",
      "Epoch 86/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 9955994238976.0000 - val_loss: 9295234072576.0000\n",
      "Epoch 87/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 8686952513536.0000 - val_loss: 8758770532352.0000\n",
      "Epoch 88/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7901251895296.0000 - val_loss: 8236311773184.0000\n",
      "Epoch 89/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 7825804230656.0000 - val_loss: 7732047380480.0000\n",
      "Epoch 90/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6979759636480.0000 - val_loss: 7241931948032.0000\n",
      "Epoch 91/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 6846877794304.0000 - val_loss: 6770101059584.0000\n",
      "Epoch 92/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 8118570844160.0000 - val_loss: 6301348265984.0000\n",
      "Epoch 93/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 5913911492608.0000 - val_loss: 5881291866112.0000\n",
      "Epoch 94/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 6687154503680.0000 - val_loss: 5460233551872.0000\n",
      "Epoch 95/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5778239913984.0000 - val_loss: 5067981193216.0000\n",
      "Epoch 96/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5290632675328.0000 - val_loss: 4705464352768.0000\n",
      "Epoch 97/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 5330240536576.0000 - val_loss: 4358012403712.0000\n",
      "Epoch 98/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4261873451008.0000 - val_loss: 4043949473792.0000\n",
      "Epoch 99/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4477509173248.0000 - val_loss: 3745212268544.0000\n",
      "Epoch 100/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3674353434624.0000 - val_loss: 3480475140096.0000\n",
      "Epoch 101/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4197439766528.0000 - val_loss: 3228704440320.0000\n",
      "Epoch 102/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3065446662144.0000 - val_loss: 3004787326976.0000\n",
      "Epoch 103/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3266486992896.0000 - val_loss: 2797830406144.0000\n",
      "Epoch 104/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2808201084928.0000 - val_loss: 2612461830144.0000\n",
      "Epoch 105/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2800445292544.0000 - val_loss: 2450632736768.0000\n",
      "Epoch 106/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2607832104960.0000 - val_loss: 2306393767936.0000\n",
      "Epoch 107/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2556786638848.0000 - val_loss: 2178000617472.0000\n",
      "Epoch 108/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2342623379456.0000 - val_loss: 2062218297344.0000\n",
      "Epoch 109/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2238839521280.0000 - val_loss: 1957684183040.0000\n",
      "Epoch 110/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2380614598656.0000 - val_loss: 1871663333376.0000\n",
      "Epoch 111/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1805497794560.0000 - val_loss: 1796130078720.0000\n",
      "Epoch 112/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1817623920640.0000 - val_loss: 1732733304832.0000\n",
      "Epoch 113/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2321735221248.0000 - val_loss: 1672491827200.0000\n",
      "Epoch 114/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1940803026944.0000 - val_loss: 1624826839040.0000\n",
      "Epoch 115/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1869381763072.0000 - val_loss: 1576877686784.0000\n",
      "Epoch 116/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1843443662848.0000 - val_loss: 1543748976640.0000\n",
      "Epoch 117/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1784264916992.0000 - val_loss: 1511190953984.0000\n",
      "Epoch 118/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1667590389760.0000 - val_loss: 1480758657024.0000\n",
      "Epoch 119/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1476730028032.0000 - val_loss: 1457060970496.0000\n",
      "Epoch 120/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2015236194304.0000 - val_loss: 1435085832192.0000\n",
      "Epoch 121/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1825672396800.0000 - val_loss: 1417619308544.0000\n",
      "Epoch 122/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1589588393984.0000 - val_loss: 1401322078208.0000\n",
      "Epoch 123/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1532488974336.0000 - val_loss: 1388123914240.0000\n",
      "Epoch 124/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1601054441472.0000 - val_loss: 1377551384576.0000\n",
      "Epoch 125/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1883506212864.0000 - val_loss: 1364824686592.0000\n",
      "Epoch 126/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1557865431040.0000 - val_loss: 1355875876864.0000\n",
      "Epoch 127/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1688141168640.0000 - val_loss: 1347308879872.0000\n",
      "Epoch 128/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1766322077696.0000 - val_loss: 1338728251392.0000\n",
      "Epoch 129/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1849309790208.0000 - val_loss: 1333790900224.0000\n",
      "Epoch 130/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1584126230528.0000 - val_loss: 1328793911296.0000\n",
      "Epoch 131/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1621068873728.0000 - val_loss: 1319784415232.0000\n",
      "Epoch 132/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1708594692096.0000 - val_loss: 1312682147840.0000\n",
      "Epoch 133/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1709065240576.0000 - val_loss: 1304091426816.0000\n",
      "Epoch 134/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1654334816256.0000 - val_loss: 1300378681344.0000\n",
      "Epoch 135/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1587859685376.0000 - val_loss: 1297405837312.0000\n",
      "Epoch 136/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1567791251456.0000 - val_loss: 1295516958720.0000\n",
      "Epoch 137/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1580331433984.0000 - val_loss: 1288490713088.0000\n",
      "Epoch 138/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1609105276928.0000 - val_loss: 1284235984896.0000\n",
      "Epoch 139/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1672554479616.0000 - val_loss: 1281905393664.0000\n",
      "Epoch 140/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1667053125632.0000 - val_loss: 1278839488512.0000\n",
      "Epoch 141/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1400560025600.0000 - val_loss: 1278855479296.0000\n",
      "Epoch 142/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1576425488384.0000 - val_loss: 1272574902272.0000\n",
      "Epoch 143/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1531518255104.0000 - val_loss: 1270976741376.0000\n",
      "Epoch 144/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1562771193856.0000 - val_loss: 1268899119104.0000\n",
      "Epoch 145/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1795972661248.0000 - val_loss: 1265829281792.0000\n",
      "Epoch 146/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1469500358656.0000 - val_loss: 1262055718912.0000\n",
      "Epoch 147/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1538391015424.0000 - val_loss: 1258618617856.0000\n",
      "Epoch 148/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1456714285056.0000 - val_loss: 1257722871808.0000\n",
      "Epoch 149/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1626842857472.0000 - val_loss: 1251787014144.0000\n",
      "Epoch 150/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1399185997824.0000 - val_loss: 1250443526144.0000\n",
      "Epoch 151/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1298560843776.0000 - val_loss: 1251047899136.0000\n",
      "Epoch 152/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1454220509184.0000 - val_loss: 1249464156160.0000\n",
      "Epoch 153/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1641430646784.0000 - val_loss: 1250095267840.0000\n",
      "Epoch 154/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1623269048320.0000 - val_loss: 1248535642112.0000\n",
      "Epoch 155/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1988463820800.0000 - val_loss: 1247584714752.0000\n",
      "Epoch 156/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1680040919040.0000 - val_loss: 1245597401088.0000\n",
      "Epoch 157/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1747258834944.0000 - val_loss: 1244718563328.0000\n",
      "Epoch 158/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1380170465280.0000 - val_loss: 1244979003392.0000\n",
      "Epoch 159/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1563655012352.0000 - val_loss: 1242457047040.0000\n",
      "Epoch 160/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1611401789440.0000 - val_loss: 1240887721984.0000\n",
      "Epoch 161/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1553516068864.0000 - val_loss: 1240181243904.0000\n",
      "Epoch 162/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1700513185792.0000 - val_loss: 1238912466944.0000\n",
      "Epoch 163/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1531655618560.0000 - val_loss: 1238863970304.0000\n",
      "Epoch 164/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1765683757056.0000 - val_loss: 1236002668544.0000\n",
      "Epoch 165/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1473638301696.0000 - val_loss: 1234125979648.0000\n",
      "Epoch 166/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1727868174336.0000 - val_loss: 1234685001728.0000\n",
      "Epoch 167/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1479359463424.0000 - val_loss: 1230320566272.0000\n",
      "Epoch 168/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1705931702272.0000 - val_loss: 1227649712128.0000\n",
      "Epoch 169/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1471320686592.0000 - val_loss: 1229883834368.0000\n",
      "Epoch 170/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1307670478848.0000 - val_loss: 1226023895040.0000\n",
      "Epoch 171/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1624007114752.0000 - val_loss: 1223945224192.0000\n",
      "Epoch 172/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1514742874112.0000 - val_loss: 1222168674304.0000\n",
      "Epoch 173/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1399631118336.0000 - val_loss: 1223301791744.0000\n",
      "Epoch 174/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1466960969728.0000 - val_loss: 1223707459584.0000\n",
      "Epoch 175/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1491470254080.0000 - val_loss: 1222638829568.0000\n",
      "Epoch 176/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1339993620480.0000 - val_loss: 1220647452672.0000\n",
      "Epoch 177/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1533464018944.0000 - val_loss: 1219372384256.0000\n",
      "Epoch 178/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1486437089280.0000 - val_loss: 1219244851200.0000\n",
      "Epoch 179/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1374644469760.0000 - val_loss: 1218008711168.0000\n",
      "Epoch 180/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1486359494656.0000 - val_loss: 1219316547584.0000\n",
      "Epoch 181/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1407438553088.0000 - val_loss: 1218265481216.0000\n",
      "Epoch 182/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1613121323008.0000 - val_loss: 1214607785984.0000\n",
      "Epoch 183/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1558408462336.0000 - val_loss: 1214032642048.0000\n",
      "Epoch 184/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1233992941568.0000 - val_loss: 1215563169792.0000\n",
      "Epoch 185/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1864301543424.0000 - val_loss: 1210753089536.0000\n",
      "Epoch 186/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1464521195520.0000 - val_loss: 1212390178816.0000\n",
      "Epoch 187/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1589768617984.0000 - val_loss: 1209852493824.0000\n",
      "Epoch 188/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1651281100800.0000 - val_loss: 1207701209088.0000\n",
      "Epoch 189/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1717407580160.0000 - val_loss: 1206042361856.0000\n",
      "Epoch 190/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1551939403776.0000 - val_loss: 1205979185152.0000\n",
      "Epoch 191/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1458152013824.0000 - val_loss: 1204507508736.0000\n",
      "Epoch 192/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1818120028160.0000 - val_loss: 1201298866176.0000\n",
      "Epoch 193/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1457105141760.0000 - val_loss: 1201605181440.0000\n",
      "Epoch 194/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1400134303744.0000 - val_loss: 1200475734016.0000\n",
      "Epoch 195/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1496650088448.0000 - val_loss: 1200677060608.0000\n",
      "Epoch 196/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1778591203328.0000 - val_loss: 1203534430208.0000\n",
      "Epoch 197/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1680403726336.0000 - val_loss: 1199952363520.0000\n",
      "Epoch 198/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1797103157248.0000 - val_loss: 1198224441344.0000\n",
      "Epoch 199/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1548155092992.0000 - val_loss: 1200763174912.0000\n",
      "Epoch 200/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1782606594048.0000 - val_loss: 1201693392896.0000\n",
      "Epoch 201/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1441852030976.0000 - val_loss: 1201109336064.0000\n",
      "Epoch 202/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1436579135488.0000 - val_loss: 1200876683264.0000\n",
      "Epoch 203/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1583553839104.0000 - val_loss: 1200726474752.0000\n",
      "Epoch 204/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1283082420224.0000 - val_loss: 1200659890176.0000\n",
      "Epoch 205/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1317521063936.0000 - val_loss: 1200294985728.0000\n",
      "Epoch 206/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1372118581248.0000 - val_loss: 1199048228864.0000\n",
      "Epoch 207/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1487298887680.0000 - val_loss: 1196312625152.0000\n",
      "Epoch 208/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1458655854592.0000 - val_loss: 1195158405120.0000\n",
      "Epoch 209/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1589274345472.0000 - val_loss: 1197158301696.0000\n",
      "Epoch 210/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1416494186496.0000 - val_loss: 1195803410432.0000\n",
      "Epoch 211/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1612477759488.0000 - val_loss: 1192458452992.0000\n",
      "Epoch 212/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1574758907904.0000 - val_loss: 1191736508416.0000\n",
      "Epoch 213/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1387678400512.0000 - val_loss: 1193671917568.0000\n",
      "Epoch 214/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1728637042688.0000 - val_loss: 1190986645504.0000\n",
      "Epoch 215/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1476518477824.0000 - val_loss: 1194524540928.0000\n",
      "Epoch 216/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1422521794560.0000 - val_loss: 1191520370688.0000\n",
      "Epoch 217/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1604014047232.0000 - val_loss: 1192277180416.0000\n",
      "Epoch 218/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1532520824832.0000 - val_loss: 1191347617792.0000\n",
      "Epoch 219/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1544771207168.0000 - val_loss: 1190417399808.0000\n",
      "Epoch 220/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1396792098816.0000 - val_loss: 1195545985024.0000\n",
      "Epoch 221/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1889053704192.0000 - val_loss: 1193109618688.0000\n",
      "Epoch 222/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1499284373504.0000 - val_loss: 1192649949184.0000\n",
      "Epoch 223/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1583207809024.0000 - val_loss: 1190042533888.0000\n",
      "Epoch 224/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1409606090752.0000 - val_loss: 1194861789184.0000\n",
      "Epoch 225/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1599859195904.0000 - val_loss: 1193491038208.0000\n",
      "Epoch 226/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1352055652352.0000 - val_loss: 1195786633216.0000\n",
      "Epoch 227/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1655346954240.0000 - val_loss: 1193364684800.0000\n",
      "Epoch 228/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1848918540288.0000 - val_loss: 1191184433152.0000\n",
      "Epoch 229/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1451405737984.0000 - val_loss: 1190134415360.0000\n",
      "Epoch 230/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1576608727040.0000 - val_loss: 1188476747776.0000\n",
      "Epoch 231/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1738704551936.0000 - val_loss: 1188954505216.0000\n",
      "Epoch 232/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1278466457600.0000 - val_loss: 1191843201024.0000\n",
      "Epoch 233/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1233624891392.0000 - val_loss: 1189158846464.0000\n",
      "Epoch 234/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1707094441984.0000 - val_loss: 1187631202304.0000\n",
      "Epoch 235/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1776043425792.0000 - val_loss: 1186338308096.0000\n",
      "Epoch 236/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1618076631040.0000 - val_loss: 1188236361728.0000\n",
      "Epoch 237/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1498710671360.0000 - val_loss: 1187535519744.0000\n",
      "Epoch 238/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1273971474432.0000 - val_loss: 1186858926080.0000\n",
      "Epoch 239/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1580000083968.0000 - val_loss: 1188145922048.0000\n",
      "Epoch 240/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1605325160448.0000 - val_loss: 1186579742720.0000\n",
      "Epoch 241/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1938839568384.0000 - val_loss: 1189332910080.0000\n",
      "Epoch 242/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1493997322240.0000 - val_loss: 1193077506048.0000\n",
      "Epoch 243/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1458362515456.0000 - val_loss: 1192350842880.0000\n",
      "Epoch 244/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1391542140928.0000 - val_loss: 1191635582976.0000\n",
      "Epoch 245/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1498627571712.0000 - val_loss: 1189990367232.0000\n",
      "Epoch 246/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1558350659584.0000 - val_loss: 1191442382848.0000\n",
      "Epoch 247/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1487786868736.0000 - val_loss: 1189280743424.0000\n",
      "Epoch 248/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1598332076032.0000 - val_loss: 1188188782592.0000\n",
      "Epoch 249/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1513408954368.0000 - val_loss: 1188389978112.0000\n",
      "Epoch 250/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1544839757824.0000 - val_loss: 1188661952512.0000\n",
      "Epoch 251/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1351482998784.0000 - val_loss: 1191607533568.0000\n",
      "Epoch 252/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1416418820096.0000 - val_loss: 1190398787584.0000\n",
      "Epoch 253/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1247251136512.0000 - val_loss: 1194086105088.0000\n",
      "Epoch 254/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1654445047808.0000 - val_loss: 1191840186368.0000\n",
      "Epoch 255/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1415797407744.0000 - val_loss: 1189843173376.0000\n",
      "Epoch 256/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1368632983552.0000 - val_loss: 1187686514688.0000\n",
      "Epoch 257/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1346693234688.0000 - val_loss: 1190185402368.0000\n",
      "Epoch 258/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1469299163136.0000 - val_loss: 1187416113152.0000\n",
      "Epoch 259/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1403657650176.0000 - val_loss: 1185154859008.0000\n",
      "Epoch 260/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1833077309440.0000 - val_loss: 1185763688448.0000\n",
      "Epoch 261/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1420697534464.0000 - val_loss: 1187432366080.0000\n",
      "Epoch 262/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1370604044288.0000 - val_loss: 1188367302656.0000\n",
      "Epoch 263/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1525718188032.0000 - val_loss: 1188835753984.0000\n",
      "Epoch 264/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1490807816192.0000 - val_loss: 1188660903936.0000\n",
      "Epoch 265/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1297669816320.0000 - val_loss: 1188388274176.0000\n",
      "Epoch 266/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1427392167936.0000 - val_loss: 1188367826944.0000\n",
      "Epoch 267/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1684551237632.0000 - val_loss: 1189378129920.0000\n",
      "Epoch 268/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1519711813632.0000 - val_loss: 1189752733696.0000\n",
      "Epoch 269/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1524117274624.0000 - val_loss: 1189274189824.0000\n",
      "Epoch 270/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1450018996224.0000 - val_loss: 1191318126592.0000\n",
      "Epoch 271/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1430843949056.0000 - val_loss: 1191553925120.0000\n",
      "Epoch 272/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1484956499968.0000 - val_loss: 1191462830080.0000\n",
      "Epoch 273/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1382812876800.0000 - val_loss: 1193734045696.0000\n",
      "Epoch 274/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1363291537408.0000 - val_loss: 1191620509696.0000\n",
      "Epoch 275/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1381285101568.0000 - val_loss: 1190829621248.0000\n",
      "Epoch 276/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1476068245504.0000 - val_loss: 1190049611776.0000\n",
      "Epoch 277/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1320748318720.0000 - val_loss: 1194241556480.0000\n",
      "Epoch 278/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1708987777024.0000 - val_loss: 1187273900032.0000\n",
      "Epoch 279/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1430786146304.0000 - val_loss: 1186621947904.0000\n",
      "Epoch 280/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1434658930688.0000 - val_loss: 1188845453312.0000\n",
      "Epoch 281/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1354495557632.0000 - val_loss: 1186074329088.0000\n",
      "Epoch 282/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1309191700480.0000 - val_loss: 1185385807872.0000\n",
      "Epoch 283/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1534838702080.0000 - val_loss: 1187837378560.0000\n",
      "Epoch 284/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1371447885824.0000 - val_loss: 1184033669120.0000\n",
      "Epoch 285/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1355361157120.0000 - val_loss: 1182390550528.0000\n",
      "Epoch 286/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1483254267904.0000 - val_loss: 1183216828416.0000\n",
      "Epoch 287/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1478768984064.0000 - val_loss: 1183871795200.0000\n",
      "Epoch 288/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1744773578752.0000 - val_loss: 1182341267456.0000\n",
      "Epoch 289/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1288996519936.0000 - val_loss: 1182763712512.0000\n",
      "Epoch 290/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1495421943808.0000 - val_loss: 1184542228480.0000\n",
      "Epoch 291/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1546376708096.0000 - val_loss: 1185126547456.0000\n",
      "Epoch 292/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1692576645120.0000 - val_loss: 1185896464384.0000\n",
      "Epoch 293/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1735963049984.0000 - val_loss: 1183841779712.0000\n",
      "Epoch 294/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1335126654976.0000 - val_loss: 1184784711680.0000\n",
      "Epoch 295/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1371351285760.0000 - val_loss: 1185694613504.0000\n",
      "Epoch 296/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1431108976640.0000 - val_loss: 1187050029056.0000\n",
      "Epoch 297/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1380185931776.0000 - val_loss: 1187814572032.0000\n",
      "Epoch 298/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1760370884608.0000 - val_loss: 1186317336576.0000\n",
      "Epoch 299/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1391052980224.0000 - val_loss: 1187955474432.0000\n",
      "Epoch 300/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1557489123328.0000 - val_loss: 1186116009984.0000\n",
      "Epoch 301/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1269572173824.0000 - val_loss: 1187214000128.0000\n",
      "Epoch 302/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1505467301888.0000 - val_loss: 1183924355072.0000\n",
      "Epoch 303/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1640259649536.0000 - val_loss: 1185578090496.0000\n",
      "Epoch 304/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1295803613184.0000 - val_loss: 1184820101120.0000\n",
      "Epoch 305/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1602615640064.0000 - val_loss: 1183218008064.0000\n",
      "Epoch 306/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1268988379136.0000 - val_loss: 1181847912448.0000\n",
      "Epoch 307/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1412122542080.0000 - val_loss: 1182130765824.0000\n",
      "Epoch 308/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1622256517120.0000 - val_loss: 1181529538560.0000\n",
      "Epoch 309/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1355283562496.0000 - val_loss: 1182081351680.0000\n",
      "Epoch 310/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1443562389504.0000 - val_loss: 1182436950016.0000\n",
      "Epoch 311/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1551185739776.0000 - val_loss: 1181199106048.0000\n",
      "Epoch 312/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1506911715328.0000 - val_loss: 1182881546240.0000\n",
      "Epoch 313/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1061779537920.0000 - val_loss: 1185320271872.0000\n",
      "Epoch 314/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1668253876224.0000 - val_loss: 1182517166080.0000\n",
      "Epoch 315/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1356059770880.0000 - val_loss: 1182904614912.0000\n",
      "Epoch 316/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1413160894464.0000 - val_loss: 1179758100480.0000\n",
      "Epoch 317/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1266297470976.0000 - val_loss: 1180452519936.0000\n",
      "Epoch 318/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1641817178112.0000 - val_loss: 1180124971008.0000\n",
      "Epoch 319/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1464929878016.0000 - val_loss: 1183562858496.0000\n",
      "Epoch 320/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1398481879040.0000 - val_loss: 1182540234752.0000\n",
      "Epoch 321/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1466341654528.0000 - val_loss: 1181572923392.0000\n",
      "Epoch 322/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1797219680256.0000 - val_loss: 1182558584832.0000\n",
      "Epoch 323/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1703520763904.0000 - val_loss: 1183957385216.0000\n",
      "Epoch 324/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1545072803840.0000 - val_loss: 1182774198272.0000\n",
      "Epoch 325/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1544872001536.0000 - val_loss: 1180588965888.0000\n",
      "Epoch 326/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1297862885376.0000 - val_loss: 1183223513088.0000\n",
      "Epoch 327/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1470894571520.0000 - val_loss: 1183473467392.0000\n",
      "Epoch 328/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1406749638656.0000 - val_loss: 1183363497984.0000\n",
      "Epoch 329/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1516478267392.0000 - val_loss: 1180385542144.0000\n",
      "Epoch 330/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1545159966720.0000 - val_loss: 1184118341632.0000\n",
      "Epoch 331/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1393249484800.0000 - val_loss: 1182824267776.0000\n",
      "Epoch 332/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1547691884544.0000 - val_loss: 1179293974528.0000\n",
      "Epoch 333/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1349028282368.0000 - val_loss: 1178167279616.0000\n",
      "Epoch 334/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1253404180480.0000 - val_loss: 1179539210240.0000\n",
      "Epoch 335/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1370382532608.0000 - val_loss: 1180388818944.0000\n",
      "Epoch 336/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1412353753088.0000 - val_loss: 1182218584064.0000\n",
      "Epoch 337/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1768627503104.0000 - val_loss: 1184214024192.0000\n",
      "Epoch 338/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1357121847296.0000 - val_loss: 1183257722880.0000\n",
      "Epoch 339/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1581815431168.0000 - val_loss: 1184159498240.0000\n",
      "Epoch 340/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1377996111872.0000 - val_loss: 1181504372736.0000\n",
      "Epoch 341/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1473376026624.0000 - val_loss: 1181838999552.0000\n",
      "Epoch 342/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1106614550528.0000 - val_loss: 1180702212096.0000\n",
      "Epoch 343/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1542048841728.0000 - val_loss: 1178917797888.0000\n",
      "Epoch 344/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1489456201728.0000 - val_loss: 1181427957760.0000\n",
      "Epoch 345/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1418967252992.0000 - val_loss: 1184197246976.0000\n",
      "Epoch 346/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1329567629312.0000 - val_loss: 1183952797696.0000\n",
      "Epoch 347/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1269825142784.0000 - val_loss: 1182959403008.0000\n",
      "Epoch 348/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1537413742592.0000 - val_loss: 1185151188992.0000\n",
      "Epoch 349/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1406697209856.0000 - val_loss: 1185958330368.0000\n",
      "Epoch 350/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1326294499328.0000 - val_loss: 1184222412800.0000\n",
      "Epoch 351/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1482778869760.0000 - val_loss: 1180593160192.0000\n",
      "Epoch 352/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1829031116800.0000 - val_loss: 1179240366080.0000\n",
      "Epoch 353/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1400718229504.0000 - val_loss: 1180954787840.0000\n",
      "Epoch 354/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1347301670912.0000 - val_loss: 1181205659648.0000\n",
      "Epoch 355/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1236091797504.0000 - val_loss: 1186524430336.0000\n",
      "Epoch 356/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1556610154496.0000 - val_loss: 1184854048768.0000\n",
      "Epoch 357/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1894385582080.0000 - val_loss: 1181638197248.0000\n",
      "Epoch 358/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1245327785984.0000 - val_loss: 1181640032256.0000\n",
      "Epoch 359/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1267839401984.0000 - val_loss: 1180711518208.0000\n",
      "Epoch 360/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1435194359808.0000 - val_loss: 1180281995264.0000\n",
      "Epoch 361/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1348935483392.0000 - val_loss: 1179296858112.0000\n",
      "Epoch 362/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1410702114816.0000 - val_loss: 1182586109952.0000\n",
      "Epoch 363/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1532789391360.0000 - val_loss: 1184825868288.0000\n",
      "Epoch 364/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1465631899648.0000 - val_loss: 1183505580032.0000\n",
      "Epoch 365/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1476443111424.0000 - val_loss: 1184947109888.0000\n",
      "Epoch 366/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1304776540160.0000 - val_loss: 1188680957952.0000\n",
      "Epoch 367/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1402293321728.0000 - val_loss: 1190709035008.0000\n",
      "Epoch 368/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1835912265728.0000 - val_loss: 1187301425152.0000\n",
      "Epoch 369/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1272711479296.0000 - val_loss: 1189857722368.0000\n",
      "Epoch 370/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1219997597696.0000 - val_loss: 1190150012928.0000\n",
      "Epoch 371/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1319644692480.0000 - val_loss: 1189350604800.0000\n",
      "Epoch 372/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1559107338240.0000 - val_loss: 1186361376768.0000\n",
      "Epoch 373/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1542101401600.0000 - val_loss: 1187807887360.0000\n",
      "Epoch 374/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1261341376512.0000 - val_loss: 1191280508928.0000\n",
      "Epoch 375/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1579078647808.0000 - val_loss: 1190550831104.0000\n",
      "Epoch 376/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1574721945600.0000 - val_loss: 1187985489920.0000\n",
      "Epoch 377/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1447294402560.0000 - val_loss: 1186161885184.0000\n",
      "Epoch 378/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1318545129472.0000 - val_loss: 1186705440768.0000\n",
      "Epoch 379/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1534187536384.0000 - val_loss: 1185645592576.0000\n",
      "Epoch 380/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1270454157312.0000 - val_loss: 1186693644288.0000\n",
      "Epoch 381/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1267433340928.0000 - val_loss: 1185115799552.0000\n",
      "Epoch 382/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1176244191232.0000 - val_loss: 1184636469248.0000\n",
      "Epoch 383/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1605595824128.0000 - val_loss: 1185559609344.0000\n",
      "Epoch 384/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1609712533504.0000 - val_loss: 1184814596096.0000\n",
      "Epoch 385/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1574369624064.0000 - val_loss: 1185493811200.0000\n",
      "Epoch 386/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1695664439296.0000 - val_loss: 1189704105984.0000\n",
      "Epoch 387/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1400026038272.0000 - val_loss: 1188119183360.0000\n",
      "Epoch 388/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1611154849792.0000 - val_loss: 1186949234688.0000\n",
      "Epoch 389/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1341805297664.0000 - val_loss: 1189020565504.0000\n",
      "Epoch 390/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1445819842560.0000 - val_loss: 1187962421248.0000\n",
      "Epoch 391/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1500480798720.0000 - val_loss: 1183883198464.0000\n",
      "Epoch 392/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2044059189248.0000 - val_loss: 1183387484160.0000\n",
      "Epoch 393/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1377964130304.0000 - val_loss: 1186038677504.0000\n",
      "Epoch 394/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1757510238208.0000 - val_loss: 1184995344384.0000\n",
      "Epoch 395/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1482624991232.0000 - val_loss: 1184872923136.0000\n",
      "Epoch 396/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1480536227840.0000 - val_loss: 1187366305792.0000\n",
      "Epoch 397/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1297675976704.0000 - val_loss: 1185106755584.0000\n",
      "Epoch 398/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1575300104192.0000 - val_loss: 1185160232960.0000\n",
      "Epoch 399/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1693075767296.0000 - val_loss: 1190002950144.0000\n",
      "Epoch 400/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1397166964736.0000 - val_loss: 1191356399616.0000\n",
      "Epoch 401/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1516641583104.0000 - val_loss: 1195238096896.0000\n",
      "Epoch 402/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1280043384832.0000 - val_loss: 1194339074048.0000\n",
      "Epoch 403/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1274290503680.0000 - val_loss: 1192847605760.0000\n",
      "Epoch 404/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1696791658496.0000 - val_loss: 1189546950656.0000\n",
      "Epoch 405/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1568163495936.0000 - val_loss: 1187746676736.0000\n",
      "Epoch 406/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1484521340928.0000 - val_loss: 1186575024128.0000\n",
      "Epoch 407/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1430566469632.0000 - val_loss: 1184632143872.0000\n",
      "Epoch 408/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1293697417216.0000 - val_loss: 1184836091904.0000\n",
      "Epoch 409/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1527999102976.0000 - val_loss: 1183234785280.0000\n",
      "Epoch 410/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1170041470976.0000 - val_loss: 1184051625984.0000\n",
      "Epoch 411/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1463372087296.0000 - val_loss: 1183504007168.0000\n",
      "Epoch 412/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1533601120256.0000 - val_loss: 1186958409728.0000\n",
      "Epoch 413/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1378580561920.0000 - val_loss: 1189706203136.0000\n",
      "Epoch 414/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1455463858176.0000 - val_loss: 1188391550976.0000\n",
      "Epoch 415/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1704167866368.0000 - val_loss: 1189379440640.0000\n",
      "Epoch 416/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1404724445184.0000 - val_loss: 1189074305024.0000\n",
      "Epoch 417/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1339449278464.0000 - val_loss: 1187350839296.0000\n",
      "Epoch 418/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1382902661120.0000 - val_loss: 1186036187136.0000\n",
      "Epoch 419/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1901643431936.0000 - val_loss: 1182873157632.0000\n",
      "Epoch 420/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1645990903808.0000 - val_loss: 1185253687296.0000\n",
      "Epoch 421/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1248166936576.0000 - val_loss: 1186685779968.0000\n",
      "Epoch 422/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1500941910016.0000 - val_loss: 1188268867584.0000\n",
      "Epoch 423/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1452750143488.0000 - val_loss: 1189726650368.0000\n",
      "Epoch 424/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1559745527808.0000 - val_loss: 1191662059520.0000\n",
      "Epoch 425/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1339728986112.0000 - val_loss: 1193005678592.0000\n",
      "Epoch 426/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1486100627456.0000 - val_loss: 1192125661184.0000\n",
      "Epoch 427/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1436848357376.0000 - val_loss: 1191598227456.0000\n",
      "Epoch 428/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1464167956480.0000 - val_loss: 1190672465920.0000\n",
      "Epoch 429/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1565457907712.0000 - val_loss: 1187937779712.0000\n",
      "Epoch 430/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1297255759872.0000 - val_loss: 1191507132416.0000\n",
      "Epoch 431/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1528744902656.0000 - val_loss: 1189751685120.0000\n",
      "Epoch 432/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1316722180096.0000 - val_loss: 1192939225088.0000\n",
      "Epoch 433/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1427038011392.0000 - val_loss: 1195229184000.0000\n",
      "Epoch 434/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1382891520000.0000 - val_loss: 1192820342784.0000\n",
      "Epoch 435/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1337791741952.0000 - val_loss: 1192501313536.0000\n",
      "Epoch 436/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1381799428096.0000 - val_loss: 1191780810752.0000\n",
      "Epoch 437/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1733000429568.0000 - val_loss: 1189845008384.0000\n",
      "Epoch 438/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1288153595904.0000 - val_loss: 1189101174784.0000\n",
      "Epoch 439/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1697579663360.0000 - val_loss: 1189066178560.0000\n",
      "Epoch 440/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1598396301312.0000 - val_loss: 1187949707264.0000\n",
      "Epoch 441/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1265120837632.0000 - val_loss: 1189685493760.0000\n",
      "Epoch 442/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1310810832896.0000 - val_loss: 1190663421952.0000\n",
      "Epoch 443/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1340661563392.0000 - val_loss: 1188672569344.0000\n",
      "Epoch 444/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1327121039360.0000 - val_loss: 1189772394496.0000\n",
      "Epoch 445/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1401837584384.0000 - val_loss: 1189634899968.0000\n",
      "Epoch 446/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1368361009152.0000 - val_loss: 1185393278976.0000\n",
      "Epoch 447/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1628234448896.0000 - val_loss: 1187179397120.0000\n",
      "Epoch 448/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1470679875584.0000 - val_loss: 1186512502784.0000\n",
      "Epoch 449/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1591044210688.0000 - val_loss: 1184136167424.0000\n",
      "Epoch 450/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1424569794560.0000 - val_loss: 1184419282944.0000\n",
      "Epoch 451/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1637920538624.0000 - val_loss: 1182167597056.0000\n",
      "Epoch 452/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1553632460800.0000 - val_loss: 1184933085184.0000\n",
      "Epoch 453/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1777046650880.0000 - val_loss: 1183474778112.0000\n",
      "Epoch 454/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1756291661824.0000 - val_loss: 1183331909632.0000\n",
      "Epoch 455/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1362806571008.0000 - val_loss: 1186087960576.0000\n",
      "Epoch 456/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1464966709248.0000 - val_loss: 1187073097728.0000\n",
      "Epoch 457/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1463443914752.0000 - val_loss: 1186277097472.0000\n",
      "Epoch 458/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1252553654272.0000 - val_loss: 1185594212352.0000\n",
      "Epoch 459/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1620819574784.0000 - val_loss: 1187497246720.0000\n",
      "Epoch 460/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1484373360640.0000 - val_loss: 1189234999296.0000\n",
      "Epoch 461/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1232158588928.0000 - val_loss: 1189507629056.0000\n",
      "Epoch 462/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1533226516480.0000 - val_loss: 1189237751808.0000\n",
      "Epoch 463/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1498351271936.0000 - val_loss: 1188453023744.0000\n",
      "Epoch 464/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1408965148672.0000 - val_loss: 1187915104256.0000\n",
      "Epoch 465/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1322579525632.0000 - val_loss: 1187274817536.0000\n",
      "Epoch 466/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1350045138944.0000 - val_loss: 1185404813312.0000\n",
      "Epoch 467/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1555498532864.0000 - val_loss: 1186673983488.0000\n",
      "Epoch 468/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1372958621696.0000 - val_loss: 1187707617280.0000\n",
      "Epoch 469/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1611644141568.0000 - val_loss: 1188547919872.0000\n",
      "Epoch 470/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1261931200512.0000 - val_loss: 1189616156672.0000\n",
      "Epoch 471/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1705173188608.0000 - val_loss: 1186803744768.0000\n",
      "Epoch 472/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1754250215424.0000 - val_loss: 1186050736128.0000\n",
      "Epoch 473/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1423387918336.0000 - val_loss: 1189965070336.0000\n",
      "Epoch 474/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1343076564992.0000 - val_loss: 1191174209536.0000\n",
      "Epoch 475/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1212662939648.0000 - val_loss: 1191373832192.0000\n",
      "Epoch 476/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1378624733184.0000 - val_loss: 1193270050816.0000\n",
      "Epoch 477/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1480490483712.0000 - val_loss: 1189682216960.0000\n",
      "Epoch 478/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1625335791616.0000 - val_loss: 1187566583808.0000\n",
      "Epoch 479/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1442707800064.0000 - val_loss: 1189156356096.0000\n",
      "Epoch 480/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1310733107200.0000 - val_loss: 1192306540544.0000\n",
      "Epoch 481/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1544985772032.0000 - val_loss: 1191490748416.0000\n",
      "Epoch 482/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1353731145728.0000 - val_loss: 1191681327104.0000\n",
      "Epoch 483/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1444026646528.0000 - val_loss: 1187963600896.0000\n",
      "Epoch 484/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1731976757248.0000 - val_loss: 1187857170432.0000\n",
      "Epoch 485/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1442667429888.0000 - val_loss: 1189469487104.0000\n",
      "Epoch 486/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1581502038016.0000 - val_loss: 1192984576000.0000\n",
      "Epoch 487/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1499490156544.0000 - val_loss: 1190745735168.0000\n",
      "Epoch 488/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1237710536704.0000 - val_loss: 1191640825856.0000\n",
      "Epoch 489/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1737521889280.0000 - val_loss: 1193578201088.0000\n",
      "Epoch 490/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1412263706624.0000 - val_loss: 1190619250688.0000\n",
      "Epoch 491/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1165955039232.0000 - val_loss: 1189942263808.0000\n",
      "Epoch 492/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1465275777024.0000 - val_loss: 1188165713920.0000\n",
      "Epoch 493/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1652167933952.0000 - val_loss: 1185214365696.0000\n",
      "Epoch 494/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1450088726528.0000 - val_loss: 1184381009920.0000\n",
      "Epoch 495/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1318980419584.0000 - val_loss: 1185229570048.0000\n",
      "Epoch 496/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1233219747840.0000 - val_loss: 1186393882624.0000\n",
      "Epoch 497/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1634297053184.0000 - val_loss: 1184471187456.0000\n",
      "Epoch 498/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1839840231424.0000 - val_loss: 1186135408640.0000\n",
      "Epoch 499/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1222327664640.0000 - val_loss: 1189506580480.0000\n",
      "Epoch 500/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1875646742528.0000 - val_loss: 1186081013760.0000\n",
      "Epoch 501/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1568824885248.0000 - val_loss: 1184982892544.0000\n",
      "Epoch 502/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1247399641088.0000 - val_loss: 1188326670336.0000\n",
      "Epoch 503/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1371105394688.0000 - val_loss: 1187429089280.0000\n",
      "Epoch 504/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1286784942080.0000 - val_loss: 1186463219712.0000\n",
      "Epoch 505/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1309521477632.0000 - val_loss: 1187425550336.0000\n",
      "Epoch 506/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1454351581184.0000 - val_loss: 1187206791168.0000\n",
      "Epoch 507/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1365171372032.0000 - val_loss: 1188547657728.0000\n",
      "Epoch 508/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1370477690880.0000 - val_loss: 1190120259584.0000\n",
      "Epoch 509/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1444408721408.0000 - val_loss: 1191493238784.0000\n",
      "Epoch 510/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1415370637312.0000 - val_loss: 1189495046144.0000\n",
      "Epoch 511/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1167979970560.0000 - val_loss: 1190653329408.0000\n",
      "Epoch 512/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1325146832896.0000 - val_loss: 1189876203520.0000\n",
      "Epoch 513/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1456735911936.0000 - val_loss: 1190441517056.0000\n",
      "Epoch 514/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1439361531904.0000 - val_loss: 1187773022208.0000\n",
      "Epoch 515/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1723025850368.0000 - val_loss: 1187794649088.0000\n",
      "Epoch 516/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1440650362880.0000 - val_loss: 1188813209600.0000\n",
      "Epoch 517/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1381188108288.0000 - val_loss: 1189705154560.0000\n",
      "Epoch 518/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1483852218368.0000 - val_loss: 1188608212992.0000\n",
      "Epoch 519/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1717148975104.0000 - val_loss: 1185736818688.0000\n",
      "Epoch 520/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1404118106112.0000 - val_loss: 1186572533760.0000\n",
      "Epoch 521/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1492380942336.0000 - val_loss: 1190245171200.0000\n",
      "Epoch 522/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1432190451712.0000 - val_loss: 1193823567872.0000\n",
      "Epoch 523/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1279914016768.0000 - val_loss: 1196369772544.0000\n",
      "Epoch 524/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1803698176000.0000 - val_loss: 1194628874240.0000\n",
      "Epoch 525/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1596498116608.0000 - val_loss: 1192614035456.0000\n",
      "Epoch 526/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1364028424192.0000 - val_loss: 1190572457984.0000\n",
      "Epoch 527/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1423249506304.0000 - val_loss: 1186960769024.0000\n",
      "Epoch 528/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1525533638656.0000 - val_loss: 1188068065280.0000\n",
      "Epoch 529/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1315468345344.0000 - val_loss: 1190008717312.0000\n",
      "Epoch 530/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1594063060992.0000 - val_loss: 1187011624960.0000\n",
      "Epoch 531/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1266160893952.0000 - val_loss: 1188781752320.0000\n",
      "Epoch 532/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1557485977600.0000 - val_loss: 1187961765888.0000\n",
      "Epoch 533/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1197130121216.0000 - val_loss: 1187875127296.0000\n",
      "Epoch 534/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1710654750720.0000 - val_loss: 1187035348992.0000\n",
      "Epoch 535/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1741255081984.0000 - val_loss: 1190302711808.0000\n",
      "Epoch 536/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1321440378880.0000 - val_loss: 1196197675008.0000\n",
      "Epoch 537/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1346158460928.0000 - val_loss: 1198714257408.0000\n",
      "Epoch 538/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1561318653952.0000 - val_loss: 1195023269888.0000\n",
      "Epoch 539/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1509257117696.0000 - val_loss: 1193609134080.0000\n",
      "Epoch 540/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1124162076672.0000 - val_loss: 1191033831424.0000\n",
      "Epoch 541/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1448287272960.0000 - val_loss: 1188124426240.0000\n",
      "Epoch 542/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1241847169024.0000 - val_loss: 1188714774528.0000\n",
      "Epoch 543/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1407017287680.0000 - val_loss: 1186712649728.0000\n",
      "Epoch 544/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1286716522496.0000 - val_loss: 1190694486016.0000\n",
      "Epoch 545/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1518049427456.0000 - val_loss: 1190698942464.0000\n",
      "Epoch 546/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1517763297280.0000 - val_loss: 1194456121344.0000\n",
      "Epoch 547/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1580761874432.0000 - val_loss: 1199631630336.0000\n",
      "Epoch 548/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1526954065920.0000 - val_loss: 1197741572096.0000\n",
      "Epoch 549/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1208921620480.0000 - val_loss: 1198908506112.0000\n",
      "Epoch 550/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1504858996736.0000 - val_loss: 1194708828160.0000\n",
      "Epoch 551/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1467110129664.0000 - val_loss: 1191763116032.0000\n",
      "Epoch 552/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1624032673792.0000 - val_loss: 1189224251392.0000\n",
      "Epoch 553/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1458267881472.0000 - val_loss: 1188385652736.0000\n",
      "Epoch 554/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1282984902656.0000 - val_loss: 1190966460416.0000\n",
      "Epoch 555/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1422301855744.0000 - val_loss: 1190798950400.0000\n",
      "Epoch 556/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1469295230976.0000 - val_loss: 1190652280832.0000\n",
      "Epoch 557/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1323338694656.0000 - val_loss: 1191584727040.0000\n",
      "Epoch 558/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1567471173632.0000 - val_loss: 1189572640768.0000\n",
      "Epoch 559/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1330492473344.0000 - val_loss: 1192760180736.0000\n",
      "Epoch 560/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1778730532864.0000 - val_loss: 1194106421248.0000\n",
      "Epoch 561/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1711802941440.0000 - val_loss: 1192560033792.0000\n",
      "Epoch 562/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1664856096768.0000 - val_loss: 1193142517760.0000\n",
      "Epoch 563/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1347765534720.0000 - val_loss: 1190535495680.0000\n",
      "Epoch 564/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1580392120320.0000 - val_loss: 1188977311744.0000\n",
      "Epoch 565/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1533764304896.0000 - val_loss: 1189272748032.0000\n",
      "Epoch 566/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1269834317824.0000 - val_loss: 1194028171264.0000\n",
      "Epoch 567/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1444170039296.0000 - val_loss: 1191924727808.0000\n",
      "Epoch 568/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1232509992960.0000 - val_loss: 1193648193536.0000\n",
      "Epoch 569/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1325378174976.0000 - val_loss: 1189280874496.0000\n",
      "Epoch 570/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1266143330304.0000 - val_loss: 1188984782848.0000\n",
      "Epoch 571/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1584699801600.0000 - val_loss: 1190343868416.0000\n",
      "Epoch 572/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1428493172736.0000 - val_loss: 1193928949760.0000\n",
      "Epoch 573/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1413313593344.0000 - val_loss: 1196886065152.0000\n",
      "Epoch 574/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1564320071680.0000 - val_loss: 1194238935040.0000\n",
      "Epoch 575/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1514260922368.0000 - val_loss: 1188728406016.0000\n",
      "Epoch 576/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1538478178304.0000 - val_loss: 1188225875968.0000\n",
      "Epoch 577/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1322895147008.0000 - val_loss: 1190526976000.0000\n",
      "Epoch 578/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1467221540864.0000 - val_loss: 1191074856960.0000\n",
      "Epoch 579/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1346044166144.0000 - val_loss: 1189169856512.0000\n",
      "Epoch 580/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1281871446016.0000 - val_loss: 1187944988672.0000\n",
      "Epoch 581/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1528518672384.0000 - val_loss: 1187910385664.0000\n",
      "Epoch 582/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1636376117248.0000 - val_loss: 1190031654912.0000\n",
      "Epoch 583/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1494294200320.0000 - val_loss: 1190713622528.0000\n",
      "Epoch 584/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1428853620736.0000 - val_loss: 1194270130176.0000\n",
      "Epoch 585/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1544590327808.0000 - val_loss: 1193767469056.0000\n",
      "Epoch 586/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1432906760192.0000 - val_loss: 1197351895040.0000\n",
      "Epoch 587/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1350018138112.0000 - val_loss: 1197981564928.0000\n",
      "Epoch 588/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1671996243968.0000 - val_loss: 1199101968384.0000\n",
      "Epoch 589/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1325230718976.0000 - val_loss: 1198235582464.0000\n",
      "Epoch 590/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1461867511808.0000 - val_loss: 1196788023296.0000\n",
      "Epoch 591/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1444023107584.0000 - val_loss: 1196808863744.0000\n",
      "Epoch 592/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1338098450432.0000 - val_loss: 1195600773120.0000\n",
      "Epoch 593/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1639122731008.0000 - val_loss: 1197492404224.0000\n",
      "Epoch 594/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1280016252928.0000 - val_loss: 1197600669696.0000\n",
      "Epoch 595/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1393170579456.0000 - val_loss: 1193961455616.0000\n",
      "Epoch 596/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1585350180864.0000 - val_loss: 1194232250368.0000\n",
      "Epoch 597/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1481110847488.0000 - val_loss: 1191575027712.0000\n",
      "Epoch 598/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1437855252480.0000 - val_loss: 1194302898176.0000\n",
      "Epoch 599/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1598339547136.0000 - val_loss: 1193543073792.0000\n",
      "Epoch 600/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1481251225600.0000 - val_loss: 1190795411456.0000\n",
      "Epoch 601/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1345487503360.0000 - val_loss: 1189624152064.0000\n",
      "Epoch 602/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1398918610944.0000 - val_loss: 1189709086720.0000\n",
      "Epoch 603/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1531880144896.0000 - val_loss: 1190024183808.0000\n",
      "Epoch 604/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1532933701632.0000 - val_loss: 1190322634752.0000\n",
      "Epoch 605/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1367616126976.0000 - val_loss: 1189461622784.0000\n",
      "Epoch 606/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1446969475072.0000 - val_loss: 1194601349120.0000\n",
      "Epoch 607/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1193231908864.0000 - val_loss: 1192550465536.0000\n",
      "Epoch 608/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1612265553920.0000 - val_loss: 1192385839104.0000\n",
      "Epoch 609/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1612171051008.0000 - val_loss: 1189105238016.0000\n",
      "Epoch 610/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1802580787200.0000 - val_loss: 1191496253440.0000\n",
      "Epoch 611/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1570354233344.0000 - val_loss: 1192863334400.0000\n",
      "Epoch 612/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1267388907520.0000 - val_loss: 1191441334272.0000\n",
      "Epoch 613/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1455450619904.0000 - val_loss: 1189659672576.0000\n",
      "Epoch 614/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1725109764096.0000 - val_loss: 1190574292992.0000\n",
      "Epoch 615/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1522045681664.0000 - val_loss: 1188191141888.0000\n",
      "Epoch 616/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1753970376704.0000 - val_loss: 1192566456320.0000\n",
      "Epoch 617/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1233380966400.0000 - val_loss: 1193591177216.0000\n",
      "Epoch 618/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1310832984064.0000 - val_loss: 1193373990912.0000\n",
      "Epoch 619/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1695232819200.0000 - val_loss: 1191424950272.0000\n",
      "Epoch 620/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1280404094976.0000 - val_loss: 1188860788736.0000\n",
      "Epoch 621/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1558291939328.0000 - val_loss: 1190177669120.0000\n",
      "Epoch 622/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1341687463936.0000 - val_loss: 1191527710720.0000\n",
      "Epoch 623/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1546744889344.0000 - val_loss: 1188345806848.0000\n",
      "Epoch 624/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1386111041536.0000 - val_loss: 1188645699584.0000\n",
      "Epoch 625/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1466603012096.0000 - val_loss: 1191482621952.0000\n",
      "Epoch 626/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1339247951872.0000 - val_loss: 1191455621120.0000\n",
      "Epoch 627/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1235386630144.0000 - val_loss: 1188228104192.0000\n",
      "Epoch 628/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1361596252160.0000 - val_loss: 1186980954112.0000\n",
      "Epoch 629/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1306985627648.0000 - val_loss: 1186960375808.0000\n",
      "Epoch 630/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1465194774528.0000 - val_loss: 1189220057088.0000\n",
      "Epoch 631/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1457051402240.0000 - val_loss: 1189689163776.0000\n",
      "Epoch 632/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1328806363136.0000 - val_loss: 1192202207232.0000\n",
      "Epoch 633/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1322324983808.0000 - val_loss: 1190528024576.0000\n",
      "Epoch 634/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1489998577664.0000 - val_loss: 1189954060288.0000\n",
      "Epoch 635/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1535614779392.0000 - val_loss: 1188982030336.0000\n",
      "Epoch 636/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1445658492928.0000 - val_loss: 1188774936576.0000\n",
      "Epoch 637/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1208791334912.0000 - val_loss: 1193729589248.0000\n",
      "Epoch 638/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1542179389440.0000 - val_loss: 1191488126976.0000\n",
      "Epoch 639/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1718993420288.0000 - val_loss: 1191139213312.0000\n",
      "Epoch 640/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1257202253824.0000 - val_loss: 1192269053952.0000\n",
      "Epoch 641/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1447015874560.0000 - val_loss: 1190518980608.0000\n",
      "Epoch 642/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1370923335680.0000 - val_loss: 1187052781568.0000\n",
      "Epoch 643/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1350407815168.0000 - val_loss: 1187431579648.0000\n",
      "Epoch 644/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1244508848128.0000 - val_loss: 1193479241728.0000\n",
      "Epoch 645/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1272041046016.0000 - val_loss: 1193141207040.0000\n",
      "Epoch 646/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1601166901248.0000 - val_loss: 1194950000640.0000\n",
      "Epoch 647/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1351306706944.0000 - val_loss: 1192391868416.0000\n",
      "Epoch 648/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1382575243264.0000 - val_loss: 1194689036288.0000\n",
      "Epoch 649/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1597792976896.0000 - val_loss: 1195362353152.0000\n",
      "Epoch 650/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1507351461888.0000 - val_loss: 1194562682880.0000\n",
      "Epoch 651/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1433824395264.0000 - val_loss: 1191413284864.0000\n",
      "Epoch 652/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1397832548352.0000 - val_loss: 1188991074304.0000\n",
      "Epoch 653/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1376060702720.0000 - val_loss: 1189854838784.0000\n",
      "Epoch 654/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1385039265792.0000 - val_loss: 1191482753024.0000\n",
      "Epoch 655/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1330174361600.0000 - val_loss: 1192496857088.0000\n",
      "Epoch 656/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1496864129024.0000 - val_loss: 1191905591296.0000\n",
      "Epoch 657/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1372148858880.0000 - val_loss: 1191075119104.0000\n",
      "Epoch 658/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1473643544576.0000 - val_loss: 1188678598656.0000\n",
      "Epoch 659/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1667533373440.0000 - val_loss: 1189607899136.0000\n",
      "Epoch 660/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1393139253248.0000 - val_loss: 1190640615424.0000\n",
      "Epoch 661/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1670904676352.0000 - val_loss: 1188492214272.0000\n",
      "Epoch 662/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1668425973760.0000 - val_loss: 1190439813120.0000\n",
      "Epoch 663/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1311834505216.0000 - val_loss: 1193122201600.0000\n",
      "Epoch 664/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1343319703552.0000 - val_loss: 1193544253440.0000\n",
      "Epoch 665/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1420933857280.0000 - val_loss: 1192698314752.0000\n",
      "Epoch 666/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1507922804736.0000 - val_loss: 1190127992832.0000\n",
      "Epoch 667/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1331740147712.0000 - val_loss: 1191761674240.0000\n",
      "Epoch 668/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1573733924864.0000 - val_loss: 1189647220736.0000\n",
      "Epoch 669/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1478139445248.0000 - val_loss: 1188051550208.0000\n",
      "Epoch 670/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1299792789504.0000 - val_loss: 1190397083648.0000\n",
      "Epoch 671/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1484329058304.0000 - val_loss: 1192882470912.0000\n",
      "Epoch 672/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1175840620544.0000 - val_loss: 1193993830400.0000\n",
      "Epoch 673/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1263590703104.0000 - val_loss: 1194332913664.0000\n",
      "Epoch 674/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1443962159104.0000 - val_loss: 1194591649792.0000\n",
      "Epoch 675/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1245196189696.0000 - val_loss: 1193429827584.0000\n",
      "Epoch 676/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1360277536768.0000 - val_loss: 1190658703360.0000\n",
      "Epoch 677/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1252281679872.0000 - val_loss: 1192763457536.0000\n",
      "Epoch 678/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1535394447360.0000 - val_loss: 1189726126080.0000\n",
      "Epoch 679/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1720616091648.0000 - val_loss: 1191657865216.0000\n",
      "Epoch 680/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1167849816064.0000 - val_loss: 1192576024576.0000\n",
      "Epoch 681/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1239902715904.0000 - val_loss: 1192483618816.0000\n",
      "Epoch 682/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1462368862208.0000 - val_loss: 1192127234048.0000\n",
      "Epoch 683/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1295196618752.0000 - val_loss: 1191990788096.0000\n",
      "Epoch 684/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1159242973184.0000 - val_loss: 1193030451200.0000\n",
      "Epoch 685/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1490361516032.0000 - val_loss: 1196247089152.0000\n",
      "Epoch 686/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1499375337472.0000 - val_loss: 1193741123584.0000\n",
      "Epoch 687/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1361266606080.0000 - val_loss: 1193045000192.0000\n",
      "Epoch 688/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1758803787776.0000 - val_loss: 1196225462272.0000\n",
      "Epoch 689/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1561766920192.0000 - val_loss: 1198695907328.0000\n",
      "Epoch 690/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1585423581184.0000 - val_loss: 1199730982912.0000\n",
      "Epoch 691/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1528422727680.0000 - val_loss: 1200681254912.0000\n",
      "Epoch 692/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1665103298560.0000 - val_loss: 1197218594816.0000\n",
      "Epoch 693/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1519120285696.0000 - val_loss: 1199402909696.0000\n",
      "Epoch 694/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1323157946368.0000 - val_loss: 1201408311296.0000\n",
      "Epoch 695/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1495996563456.0000 - val_loss: 1201042620416.0000\n",
      "Epoch 696/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1350894616576.0000 - val_loss: 1201799692288.0000\n",
      "Epoch 697/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1520092315648.0000 - val_loss: 1201858019328.0000\n",
      "Epoch 698/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1297077239808.0000 - val_loss: 1200682696704.0000\n",
      "Epoch 699/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1257873211392.0000 - val_loss: 1199053340672.0000\n",
      "Epoch 700/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1350704168960.0000 - val_loss: 1200206643200.0000\n",
      "Epoch 701/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1428344274944.0000 - val_loss: 1199671869440.0000\n",
      "Epoch 702/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1567522816000.0000 - val_loss: 1199359655936.0000\n",
      "Epoch 703/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1510519734272.0000 - val_loss: 1199815524352.0000\n",
      "Epoch 704/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1329078599680.0000 - val_loss: 1194799923200.0000\n",
      "Epoch 705/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1320647524352.0000 - val_loss: 1192888762368.0000\n",
      "Epoch 706/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1549667008512.0000 - val_loss: 1193148678144.0000\n",
      "Epoch 707/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1420185305088.0000 - val_loss: 1194257940480.0000\n",
      "Epoch 708/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1364200652800.0000 - val_loss: 1195785322496.0000\n",
      "Epoch 709/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1204147322880.0000 - val_loss: 1195076485120.0000\n",
      "Epoch 710/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1523787628544.0000 - val_loss: 1193020096512.0000\n",
      "Epoch 711/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1434413826048.0000 - val_loss: 1191775305728.0000\n",
      "Epoch 712/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1096353054720.0000 - val_loss: 1189948293120.0000\n",
      "Epoch 713/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1377912487936.0000 - val_loss: 1192400781312.0000\n",
      "Epoch 714/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1439504269312.0000 - val_loss: 1190945226752.0000\n",
      "Epoch 715/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1300815937536.0000 - val_loss: 1192467628032.0000\n",
      "Epoch 716/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1399905583104.0000 - val_loss: 1194243391488.0000\n",
      "Epoch 717/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1684201799680.0000 - val_loss: 1195677057024.0000\n",
      "Epoch 718/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1587936624640.0000 - val_loss: 1192653881344.0000\n",
      "Epoch 719/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1443275866112.0000 - val_loss: 1191504117760.0000\n",
      "Epoch 720/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1277672292352.0000 - val_loss: 1191963525120.0000\n",
      "Epoch 721/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1547670781952.0000 - val_loss: 1190756089856.0000\n",
      "Epoch 722/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1500742942720.0000 - val_loss: 1190414647296.0000\n",
      "Epoch 723/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1521826398208.0000 - val_loss: 1191118766080.0000\n",
      "Epoch 724/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1486471299072.0000 - val_loss: 1192287141888.0000\n",
      "Epoch 725/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1411049062400.0000 - val_loss: 1196386811904.0000\n",
      "Epoch 726/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1285164236800.0000 - val_loss: 1192829517824.0000\n",
      "Epoch 727/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1191389822976.0000 - val_loss: 1192174026752.0000\n",
      "Epoch 728/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1595586641920.0000 - val_loss: 1189609209856.0000\n",
      "Epoch 729/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1250090024960.0000 - val_loss: 1191619985408.0000\n",
      "Epoch 730/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 1555048038400.0000 - val_loss: 1191510671360.0000\n",
      "Epoch 731/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1402858504192.0000 - val_loss: 1195334303744.0000\n",
      "Epoch 732/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1434516324352.0000 - val_loss: 1197415071744.0000\n",
      "Epoch 733/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1317164023808.0000 - val_loss: 1197369851904.0000\n",
      "Epoch 734/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1322123526144.0000 - val_loss: 1197904101376.0000\n",
      "Epoch 735/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1647316303872.0000 - val_loss: 1195558830080.0000\n",
      "Epoch 736/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1474910486528.0000 - val_loss: 1190745997312.0000\n",
      "Epoch 737/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1296764502016.0000 - val_loss: 1193102147584.0000\n",
      "Epoch 738/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1552066412544.0000 - val_loss: 1190851248128.0000\n",
      "Epoch 739/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1418995957760.0000 - val_loss: 1191565983744.0000\n",
      "Epoch 740/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1346271313920.0000 - val_loss: 1190796722176.0000\n",
      "Epoch 741/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1288440774656.0000 - val_loss: 1190342164480.0000\n",
      "Epoch 742/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1483462934528.0000 - val_loss: 1190791479296.0000\n",
      "Epoch 743/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1505515536384.0000 - val_loss: 1193690267648.0000\n",
      "Epoch 744/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1164968198144.0000 - val_loss: 1194246799360.0000\n",
      "Epoch 745/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1393501011968.0000 - val_loss: 1196471353344.0000\n",
      "Epoch 746/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1470624169984.0000 - val_loss: 1193972727808.0000\n",
      "Epoch 747/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1444963418112.0000 - val_loss: 1195989794816.0000\n",
      "Epoch 748/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1422447738880.0000 - val_loss: 1193972596736.0000\n",
      "Epoch 749/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1546016915456.0000 - val_loss: 1192440889344.0000\n",
      "Epoch 750/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1455732817920.0000 - val_loss: 1190000066560.0000\n",
      "Epoch 751/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1480126365696.0000 - val_loss: 1191581974528.0000\n",
      "Epoch 752/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1424120348672.0000 - val_loss: 1194234871808.0000\n",
      "Epoch 753/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1358767980544.0000 - val_loss: 1195490934784.0000\n",
      "Epoch 754/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1442314321920.0000 - val_loss: 1196506087424.0000\n",
      "Epoch 755/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1320426143744.0000 - val_loss: 1196362563584.0000\n",
      "Epoch 756/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1508312743936.0000 - val_loss: 1192868184064.0000\n",
      "Epoch 757/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1372537356288.0000 - val_loss: 1195423432704.0000\n",
      "Epoch 758/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1546057809920.0000 - val_loss: 1194657447936.0000\n",
      "Epoch 759/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1280097779712.0000 - val_loss: 1195120525312.0000\n",
      "Epoch 760/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1545153675264.0000 - val_loss: 1192149254144.0000\n",
      "Epoch 761/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1486535917568.0000 - val_loss: 1191486029824.0000\n",
      "Epoch 762/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1398304145408.0000 - val_loss: 1195733024768.0000\n",
      "Epoch 763/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1644668256256.0000 - val_loss: 1195045945344.0000\n",
      "Epoch 764/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1457072504832.0000 - val_loss: 1192160264192.0000\n",
      "Epoch 765/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1365797896192.0000 - val_loss: 1193695903744.0000\n",
      "Epoch 766/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1184399884288.0000 - val_loss: 1195666571264.0000\n",
      "Epoch 767/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1234760761344.0000 - val_loss: 1195207819264.0000\n",
      "Epoch 768/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1416696430592.0000 - val_loss: 1196831408128.0000\n",
      "Epoch 769/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1580166807552.0000 - val_loss: 1193880453120.0000\n",
      "Epoch 770/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1313253490688.0000 - val_loss: 1195512954880.0000\n",
      "Epoch 771/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1569087160320.0000 - val_loss: 1197251887104.0000\n",
      "Epoch 772/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1252816453632.0000 - val_loss: 1196632702976.0000\n",
      "Epoch 773/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1310405296128.0000 - val_loss: 1194664263680.0000\n",
      "Epoch 774/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1507780591616.0000 - val_loss: 1195341512704.0000\n",
      "Epoch 775/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1395076759552.0000 - val_loss: 1194414964736.0000\n",
      "Epoch 776/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1557053046784.0000 - val_loss: 1194684448768.0000\n",
      "Epoch 777/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1388935249920.0000 - val_loss: 1193644654592.0000\n",
      "Epoch 778/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1383958315008.0000 - val_loss: 1193304784896.0000\n",
      "Epoch 779/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1223458947072.0000 - val_loss: 1193705734144.0000\n",
      "Epoch 780/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1167632891904.0000 - val_loss: 1194111139840.0000\n",
      "Epoch 781/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1639359447040.0000 - val_loss: 1193715564544.0000\n",
      "Epoch 782/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1457421680640.0000 - val_loss: 1195191697408.0000\n",
      "Epoch 783/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1420738953216.0000 - val_loss: 1192837513216.0000\n",
      "Epoch 784/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1344705789952.0000 - val_loss: 1193599303680.0000\n",
      "Epoch 785/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1331370262528.0000 - val_loss: 1193234399232.0000\n",
      "Epoch 786/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1192913403904.0000 - val_loss: 1192711159808.0000\n",
      "Epoch 787/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1589745549312.0000 - val_loss: 1191318388736.0000\n",
      "Epoch 788/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1316973445120.0000 - val_loss: 1193166897152.0000\n",
      "Epoch 789/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1609828401152.0000 - val_loss: 1192631992320.0000\n",
      "Epoch 790/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1370929496064.0000 - val_loss: 1194430562304.0000\n",
      "Epoch 791/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1664204013568.0000 - val_loss: 1192829124608.0000\n",
      "Epoch 792/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1302387228672.0000 - val_loss: 1194345889792.0000\n",
      "Epoch 793/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1698596519936.0000 - val_loss: 1195693309952.0000\n",
      "Epoch 794/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1411274899456.0000 - val_loss: 1196541870080.0000\n",
      "Epoch 795/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1224310652928.0000 - val_loss: 1197970292736.0000\n",
      "Epoch 796/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1275684847616.0000 - val_loss: 1198585020416.0000\n",
      "Epoch 797/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1405193945088.0000 - val_loss: 1193612804096.0000\n",
      "Epoch 798/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1365242019840.0000 - val_loss: 1197721518080.0000\n",
      "Epoch 799/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1284276092928.0000 - val_loss: 1196126240768.0000\n",
      "Epoch 800/800\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1319483867136.0000 - val_loss: 1194399891456.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18cc8325b70>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=800, validation_data=(X_val, y_val),callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_df = pd.DataFrame(model.history.history)\n",
    "loss_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=best_model_regression2.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model_regression2.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\janni\\OneDrive\\Fächer - Uni\\5.Semester\\DeepL\\Test\\.venv\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:198\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    195\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m     )\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File not found: filepath=best_model_regression2.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('best_model_regression2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data evaluation:\n",
      "1754523762688.0\n",
      "\n",
      "Train data evaluation:\n",
      "1039155003392.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Test data evaluation:\")\n",
    "print(model.evaluate(X_test, y_test, verbose=0))\n",
    "print(\"\\nTrain data evaluation:\")\n",
    "print(model.evaluate(X_train, y_train, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test True Y</th>\n",
       "      <th>Model Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4340000</td>\n",
       "      <td>4339588.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4403000</td>\n",
       "      <td>4111337.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4095000</td>\n",
       "      <td>3798822.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8295000</td>\n",
       "      <td>6502436.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7343000</td>\n",
       "      <td>6858161.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3150000</td>\n",
       "      <td>5400091.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8890000</td>\n",
       "      <td>6127311.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3780000</td>\n",
       "      <td>4466423.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4690000</td>\n",
       "      <td>4032625.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6293000</td>\n",
       "      <td>6613077.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2100000</td>\n",
       "      <td>3029234.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4060000</td>\n",
       "      <td>3267967.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4095000</td>\n",
       "      <td>4441747.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4375000</td>\n",
       "      <td>3169994.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8575000</td>\n",
       "      <td>7067301.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4970000</td>\n",
       "      <td>6514966.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3430000</td>\n",
       "      <td>2711374.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5950000</td>\n",
       "      <td>4591018.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9310000</td>\n",
       "      <td>6924481.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7910000</td>\n",
       "      <td>7081478.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6440000</td>\n",
       "      <td>7265127.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7000000</td>\n",
       "      <td>7078546.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3703000</td>\n",
       "      <td>4436045.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6230000</td>\n",
       "      <td>6124669.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5215000</td>\n",
       "      <td>4813994.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3990000</td>\n",
       "      <td>4247393.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5495000</td>\n",
       "      <td>4698203.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3290000</td>\n",
       "      <td>3662730.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3773000</td>\n",
       "      <td>4766435.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4480000</td>\n",
       "      <td>4209906.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3395000</td>\n",
       "      <td>5432041.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4200000</td>\n",
       "      <td>5744769.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4900000</td>\n",
       "      <td>4982018.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4305000</td>\n",
       "      <td>5557038.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6160000</td>\n",
       "      <td>6352061.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>7455000</td>\n",
       "      <td>4820969.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>7140000</td>\n",
       "      <td>5411855.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>11410000</td>\n",
       "      <td>6441251.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4795000</td>\n",
       "      <td>7373432.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5950000</td>\n",
       "      <td>5617904.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3010000</td>\n",
       "      <td>2625460.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>6615000</td>\n",
       "      <td>5995347.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>5873000</td>\n",
       "      <td>5656636.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4200000</td>\n",
       "      <td>4151168.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>6020000</td>\n",
       "      <td>4841674.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2590000</td>\n",
       "      <td>2559288.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5250000</td>\n",
       "      <td>4862688.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5950000</td>\n",
       "      <td>6311004.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3360000</td>\n",
       "      <td>2852134.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5600000</td>\n",
       "      <td>5154995.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3150000</td>\n",
       "      <td>3793247.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4550000</td>\n",
       "      <td>3653876.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>5866000</td>\n",
       "      <td>3486194.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3150000</td>\n",
       "      <td>2602287.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6195000</td>\n",
       "      <td>6089737.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Test True Y  Model Predictions\n",
       "0       4340000         4339588.00\n",
       "1       4403000         4111337.25\n",
       "2       4095000         3798822.00\n",
       "3       8295000         6502436.00\n",
       "4       7343000         6858161.50\n",
       "5       3150000         5400091.00\n",
       "6       8890000         6127311.00\n",
       "7       3780000         4466423.50\n",
       "8       4690000         4032625.25\n",
       "9       6293000         6613077.00\n",
       "10      2100000         3029234.00\n",
       "11      4060000         3267967.25\n",
       "12      4095000         4441747.00\n",
       "13      4375000         3169994.75\n",
       "14      8575000         7067301.00\n",
       "15      4970000         6514966.00\n",
       "16      3430000         2711374.50\n",
       "17      5950000         4591018.00\n",
       "18      9310000         6924481.00\n",
       "19      7910000         7081478.00\n",
       "20      6440000         7265127.50\n",
       "21      7000000         7078546.50\n",
       "22      3703000         4436045.00\n",
       "23      6230000         6124669.00\n",
       "24      5215000         4813994.00\n",
       "25      3990000         4247393.00\n",
       "26      5495000         4698203.00\n",
       "27      3290000         3662730.75\n",
       "28      3773000         4766435.50\n",
       "29      4480000         4209906.50\n",
       "30      3395000         5432041.00\n",
       "31      4200000         5744769.50\n",
       "32      4900000         4982018.00\n",
       "33      4305000         5557038.00\n",
       "34      6160000         6352061.00\n",
       "35      7455000         4820969.00\n",
       "36      7140000         5411855.00\n",
       "37     11410000         6441251.00\n",
       "38      4795000         7373432.00\n",
       "39      5950000         5617904.50\n",
       "40      3010000         2625460.00\n",
       "41      6615000         5995347.50\n",
       "42      5873000         5656636.00\n",
       "43      4200000         4151168.25\n",
       "44      6020000         4841674.50\n",
       "45      2590000         2559288.25\n",
       "46      5250000         4862688.00\n",
       "47      5950000         6311004.50\n",
       "48      3360000         2852134.75\n",
       "49      5600000         5154995.00\n",
       "50      3150000         3793247.75\n",
       "51      4550000         3653876.75\n",
       "52      5866000         3486194.50\n",
       "53      3150000         2602287.25\n",
       "54      6195000         6089737.50"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# reshape the data for easier comparison table\n",
    "test_predictions = pd.Series(test_predictions.reshape(len(y_test),))\n",
    "pred_df = pd.DataFrame(np.asarray(y_test), columns=['Test True Y'])\n",
    "pred_df = pd.concat([pred_df, test_predictions], axis=1)\n",
    "pred_df.columns = ['Test True Y', 'Model Predictions']\n",
    "\n",
    "# print the comparison table - true values vs. model predicted values\n",
    "# we can nicely see here how far off our model is in some cases\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Test True Y', ylabel='Model Predictions'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x='Test True Y', y='Model Predictions', data=pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE\n",
      "947168.96 Unit\n",
      "\n",
      "MSE\n",
      "1754523789323.62 Unit^2\n",
      "\n",
      "RMSE:\n",
      "1324584.38 Unit\n",
      "\n",
      "R-squared:\n",
      "0.49\n",
      "\n",
      "Explained variance score:\n",
      "0.52\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"MAE\")\n",
    "print(round(metrics.mean_absolute_error(y_test, test_predictions), 2), \"Unit\")\n",
    "\n",
    "print(\"\\nMSE\")\n",
    "print(round(metrics.mean_squared_error(y_test, test_predictions), 2), \"Unit^2\")\n",
    "\n",
    "print('\\nRMSE:')\n",
    "print(round(np.sqrt(metrics.mean_squared_error(y_test, test_predictions)), 2), \"Unit\")\n",
    "\n",
    "\n",
    "print('\\nR-squared:')\n",
    "print(round(metrics.r2_score(y_test, test_predictions), 2))\n",
    "\n",
    "\n",
    "print(\"\\nExplained variance score:\")\n",
    "print(round(metrics.explained_variance_score(y_test, test_predictions), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "\n",
      " Predictions \n",
      "\n",
      "82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janni\\AppData\\Local\\Temp\\ipykernel_1500\\1872061308.py:4: UserWarning: \n",
      "\n",
      "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
      "\n",
      "Please adapt your code to use either `displot` (a figure-level function with\n",
      "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "\n",
      "For a guide to updating your code to use the new functions, please see\n",
      "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
      "\n",
      "  sns.distplot((y_test - test_predictions))\n",
      "c:\\Users\\janni\\OneDrive\\Fächer - Uni\\5.Semester\\DeepL\\Test\\.venv\\lib\\site-packages\\seaborn\\distributions.py:2464: RuntimeWarning: Mean of empty slice.\n",
      "  line, = ax.plot(a.mean(), 0)\n",
      "c:\\Users\\janni\\OneDrive\\Fächer - Uni\\5.Semester\\DeepL\\Test\\.venv\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Users\\janni\\OneDrive\\Fächer - Uni\\5.Semester\\DeepL\\Test\\.venv\\lib\\site-packages\\numpy\\lib\\histograms.py:885: RuntimeWarning: invalid value encountered in divide\n",
      "  return n/db/n.sum(), bin_edges\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(y_test))\n",
    "print(\"\\n Predictions \\n\")\n",
    "print(len(test_predictions))\n",
    "sns.distplot((y_test - test_predictions))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus_furnished</th>\n",
       "      <th>furnishingstatus_semi-furnished</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13300000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12250000</td>\n",
       "      <td>8960</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12250000</td>\n",
       "      <td>9960</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12215000</td>\n",
       "      <td>7500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11410000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price  area  bedrooms  bathrooms  stories  mainroad  guestroom  \\\n",
       "0  13300000  7420         4          2        3         1          0   \n",
       "1  12250000  8960         4          4        4         1          0   \n",
       "2  12250000  9960         3          2        2         1          0   \n",
       "3  12215000  7500         4          2        2         1          0   \n",
       "4  11410000  7420         4          1        2         1          1   \n",
       "\n",
       "   basement  hotwaterheating  airconditioning  parking  prefarea  \\\n",
       "0         0                0                1        2         1   \n",
       "1         0                0                1        3         0   \n",
       "2         1                0                0        2         1   \n",
       "3         1                0                1        3         1   \n",
       "4         1                0                1        2         0   \n",
       "\n",
       "   furnishingstatus_furnished  furnishingstatus_semi-furnished  \n",
       "0                           1                                0  \n",
       "1                           1                                0  \n",
       "2                           0                                1  \n",
       "3                           1                                0  \n",
       "4                           1                                0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_row = {\n",
    "'area': 7420, \n",
    "'bedrooms':4, \n",
    "'bathrooms':2, \n",
    "'stories':3, \n",
    "'mainroad':1,\n",
    "'guestroom':0,\n",
    "'basement':0, \n",
    "'hotwaterheating':0, \n",
    "'airconditioning':1,\n",
    "'parking':2, \n",
    "'prefarea':1, \n",
    "'furnishingstatus_furnished':1,\n",
    "'furnishingstatus_semi-furnished':0\n",
    "}\n",
    "\n",
    "# convert to pandas-format\n",
    "tester_row = pd.DataFrame([tester_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "Estimated Price:\n",
      "8102746.0\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janni\\AppData\\Local\\Temp\\ipykernel_1500\\2460745286.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(f\"{round(float(result), 2)}\")\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(tester_row)[0]\n",
    "\n",
    "print()\n",
    "print(f\"Estimated Price:\")\n",
    "print(f\"{round(float(result), 2)}\")\n",
    "print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
